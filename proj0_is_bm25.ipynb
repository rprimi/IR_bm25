{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPA4On61ZC9I4VC+3MfPCEV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rprimi/IR_bm25/blob/main/proj0_is_bm25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Building a Simple Information Retrieval System using BM25\n",
        "\n",
        "## Description\n",
        "\n",
        "Exercise for special student selection for the Course \"Deep Learning applied to search systems\", FEEC-Unicamp.\n",
        "\n",
        "**Objectives:** \n",
        "- Create a basic system of Information Retrieval (IR) using the BM25 ranking algorithm in Python.\n",
        "\n",
        "**Deliverables:**\n",
        "- A github repository\n",
        "- Google Colab notebooks and associated functions and libraries if any, with the code for the IR system\n",
        "- A brief report describing the implementation details, results, how to test the IR system and how chatGPT helped you with the project.\n",
        "- Evaluation of the model in the CISI collection\n",
        "http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/\n",
        "\n",
        "**Grading:**  \n",
        "The project will be evaluated based on the following criteria:\n",
        "- Completion of the project deliverables (partial is OK to submit)\n",
        "- Implementation accuracy\n",
        "- Quality of the report\n",
        "- Code readability and commenting\n",
        "- How chatGPT was used\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZfrttcD9tn85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning about BM25\n",
        "\n",
        "BM25 is a ranking function used by search engines to estimate the relevance of documents for a given search query¹². It is based on the probabilistic information retrieval model developed in the 1970s and 1980s by Stephen E. Robertson, Karen Spärck Jones and others².\n",
        "\n",
        "BM25 is a function that scores a set of documents based on the query terms that appear in each document, regardless of how close the terms are within the document². It is a family of scoring functions with slightly different components and parameters. One of the most prominent instances of the function is the following²:\n",
        "\n",
        "$$\n",
        "\\mathrm{score}(D,Q) = \\sum_{i=1}^{n} \\mathrm{IDF}(q_i) \\cdot \\frac{f(q_i,D) \\cdot (k_1 + 1)}{f(q_i,D) + k_1 \\cdot (1 - b + b \\cdot D / \\mathrm{avgdl})}\n",
        "$$\n",
        "\n",
        "\n",
        "Where:\n",
        "- $score(D,Q)$ is the BM25 score of document $D$ for query $Q$\n",
        "- $\\sum_{i=1}^{n}$ is the sum over all query terms $q_i$ that appear in document $D$\n",
        "- The $IDF(q_i)$ is given by:\n",
        "\n",
        "$$\\mathrm{IDF}(q_i) = \\log{_e} \\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}$$\n",
        "- where:\n",
        "  - $N$ is the total number of documents in the collection\n",
        "  - $n(q_i)$ is the number of documents that contain the term $q_i$\n",
        "  - The smaller the value of $n(q_i)$, the greater the value of $IDF(q_i)$ and the greater the contribution of the term to the final score. The IDF increases as the term becomes rarer in the collection, and decreases as it becomes more common. \"The IDF component of our formula measures how often a term occurs in all of the documents and “penalizes” terms that are common\"\n",
        "- $f(q_i,D)$ is the frequency of the term $q_i$ in the document $D$, that is, how many times it appears in the text. The higher this value, the more relevant the document is to the term.\n",
        "- $D$ is the length of the document D in words. $\\mathrm{avgdl}$ is the average length of the documents in the collection. The length of the documento is divided by the average doc length in the denominator: $D / \\mathrm{avgdl}$ .\"The way to think about this is that the more terms in the document — at least ones not matching the query — the lower the score for the document\"²\n",
        "- $b$ controls how much the score is affected by document length relative to the average length of documents in the collection. Larger values of $b$ favor shorter documents, while smaller values favor longer documents. The commonly used default value is $b = 0.75$. \"if b is bigger, the effects of the length of the document compared to the average length are more amplified.\"\n",
        "\n",
        "- $(k_1 + 1)$ and $(k_1 * (1 - b + b * |D| / avgdl))$ are two components that adjust for the influence of term frequency on the final score. They depend on the free parameters:\n",
        "- $k_1$ controls how much the score increases based on term frequency. Larger values of $k_1$ mean that differences in term frequency have more impact on the score, while smaller values mean that differences have less impact. The commonly used default value is $k_1 = 1.2 to 2.0$.\n",
        "These elements together form a ranking function that attempts to estimate how relevant a document is to a query based on the terms they share.\n",
        "\n",
        "### Referências\n",
        "(1) Okapi BM25 - Wikipedia. https://en.wikipedia.org/wiki/Okapi_BM25 Acessado 16/02/2023.  \n",
        "(2) Practical BM25 - Part 2: The BM25 Algorithm and its Variables. https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables Acessado 16/02/2023.  \n",
        "(3) Document similarities with BM25 algorithm - MATLAB bm25Similarity. https://www.mathworks.com/help/textanalytics/ref/bm25similarity.html Acessado 16/02/2023.  \n",
        "(3) python-bm25 | Python implementation of BM25 function | Search Engine .... https://kandi.openweaver.com/python/fanta-mnix/python-bm25#:~:text=python-bm25%20is%20a%20Python%20library%20typically%20used%20in,Python%20implementation%20of%20BM25%20function%20for%20document%20retrieval Acessado 16/02/2023.  \n",
        "(4) BM25 using Python Gensim Package | Search Engine. https://iamgeekydude.com/2022/12/25/bm25-using-python-gensim-package-search-engine-nlp/ Acessado 16/02/2023.  \n",
        "(5) GitHub - nhirakawa/BM25: A Python implementation of the BM25 ranking .... https://github.com/nhirakawa/BM25 Acessado 16/02/2023.  \n",
        "(6) GitHub - fanta-mnix/python-bm25: Python implementation of BM25 function .... https://github.com/fanta-mnix/python-bm25 Acessado 16/02/2023.  \n",
        "(7) GitHub - xianchen2/Text_Retrieval_BM25: Python implementation of the .... https://github.com/xianchen2/Text_Retrieval_BM25 Acessado 16/02/2023.  \n",
        "(8) Machine Learning Approach for Improved BM25 Retrieval. https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/LearningBM25MSRTechReport.pdf Acessado 16/02/2023.  \n",
        "(9) Information retrieval evaluation of precision, recall, f-score, AP, MAP .... https://stackoverflow.com/questions/40457331/information-retrieval-evaluation-of-precision-recall-f-score-ap-map-in-pytho Acessado 16/02/2023.  \n",
        "(10) Information Retrieval with document Re-ranking with BERT and BM25. https://medium.com/@papai143/information-retrieval-with-document-re-ranking-with-bert-and-bm25-7c29d738df73 Acessado 16/02/2023.  \n",
        "(11) python-bm25 | Python implementation of BM25 function | Search Engine .... https://kandi.openweaver.com/python/fanta-mnix/python-bm25 Acessado 16/02/2023.  "
      ],
      "metadata": {
        "id": "Xym-jV_QvbwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo \"baunilha\" :)"
      ],
      "metadata": {
        "id": "t4FATEmPLxbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25\n",
        "import rank_bm25\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Cria documentos como lista de palavras\n",
        "docs = [\n",
        "    [\"black\", \"cat\", \"white\", \"cat\"],\n",
        "    [\"cat\", \"outer\", \"space\", \"cat\"],\n",
        "    [\"wag\", \"dog\"]\n",
        "]\n",
        "\n",
        "# Criar um objeto BM25Okapi com os documentos\n",
        "bm25 = BM25Okapi(docs)\n",
        "\n",
        "\n",
        "# Cria uma consulta como lista de palavras\n",
        "query = [[\"cat\"], [\"cat\", \"house\"]]\n",
        "\n",
        "# Calcular as pontuações dos documentos para a consulta usando get_scores()\n",
        "scores = [bm25.get_scores(query = q) for q in query] \n",
        "\n",
        "# Imprimir as pontuações\n",
        "\n",
        "print(scores)\n",
        "\n"
      ],
      "metadata": {
        "id": "PbICSYSXv91_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementração do algotitmo BM25 com avaliação na coleção CISI usando as bibliotecas gensin e nltk\n",
        "\n",
        "- Database description (from ChatGPT)\n",
        "\n",
        "  - The CISI (Computer and Information Science Index) database is a classic test collection widely used in the field of information retrieval for research and evaluation purposes. The database consists of a collection of documents and a set of queries, as well as relevance judgments for each query-document pair.\n",
        "\n",
        "  - The CISI database is available in several formats, including plain text, SGML, and XML. The most commonly used format is the plain text format, which consists of the following files:\n",
        "\n",
        "  - cisi.all - This file contains the full text of all the documents in the collection. Each document is separated by a line starting with the tag .I followed by a unique document ID. The text of the document follows on subsequent lines until the next .I tag.\n",
        "\n",
        "  - cisi.docs - This file contains bibliographic information for each document in the collection. Each record begins with the tag .I followed by the document ID. The bibliographic information is contained in fields such as .T (title), .A (author), .B (source), and .W (abstract). The fields are separated by the tag and enclosed in angle brackets, like <.T>, <.A>, etc.\n",
        "\n",
        "  - cisi.qry - This file contains the queries in the collection. Each query is preceded by the tag .I followed by a unique query ID. The text of the query follows on subsequent lines until the next .I tag.\n",
        "\n",
        "  - cisi.rel - This file contains the relevance judgments for each query-document pair. Each record begins with the tag .I followed by the query ID, the tag .R and the document ID of a relevant document. Multiple relevant documents are listed on subsequent lines until the next .I tag.\n",
        "\n",
        "- In summary, the CISI database has four main files: cisi.all for the document text, cisi.docs for document metadata, cisi.qry for queries, and cisi.rel for relevance judgments. The structure of each file is defined by a set of tags, which are used to identify different parts of the data.\n"
      ],
      "metadata": {
        "id": "dMcTAb2rMKVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Cria diretório no google drive\n",
        "!mkdir -p /content/drive/MyDrive/cisi\n",
        "\n",
        "# Baixa base\n",
        "!wget -P /content/drive/MyDrive/cisi/ http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/cisi.tar.gz\n",
        "\n",
        "# Descomprime arquivos\n",
        "!tar -xvzf /content/drive/MyDrive/cisi/cisi.tar.gz -C /content/drive/MyDrive/cisi/\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTLhxOjsRyIO",
        "outputId": "2d1099a2-4b9e-4191-ee02-b25d1ac4c94a"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "--2023-02-21 17:21:18--  http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/cisi.tar.gz\n",
            "Resolving ir.dcs.gla.ac.uk (ir.dcs.gla.ac.uk)... 130.209.240.253\n",
            "Connecting to ir.dcs.gla.ac.uk (ir.dcs.gla.ac.uk)|130.209.240.253|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 775144 (757K) [application/gzip]\n",
            "Saving to: ‘/content/drive/MyDrive/cisi/cisi.tar.gz’\n",
            "\n",
            "cisi.tar.gz         100%[===================>] 756.98K  1.26MB/s    in 0.6s    \n",
            "\n",
            "2023-02-21 17:21:19 (1.26 MB/s) - ‘/content/drive/MyDrive/cisi/cisi.tar.gz’ saved [775144/775144]\n",
            "\n",
            "CISI.ALL\n",
            "CISI.BLN\n",
            "CISI.QRY\n",
            "CISI.REL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instala biblioteca gensim, que fornece uma implementação do BM25 em Python\n",
        "!pip install gensim\n",
        "!pip install nltk\n",
        "!pip install rank_bm25\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from gensim.summarization import bm25\n",
        "\n",
        "\n",
        "def read_cisi_file(filename):\n",
        "  # Abre o arquivo no modo leitura\n",
        "  with open(filename, \"r\") as f:\n",
        "    # Lê todo o conteúdo do arquivo\n",
        "    content = f.read()\n",
        "    # Divide o conteúdo em registros separados pelo caractere \".I\"\n",
        "    records = content.split(\".I\")\n",
        "    # Remove o primeiro registro vazio\n",
        "    records.pop(0)\n",
        "    # Cria uma lista vazia para armazenar os textos dos registros\n",
        "    texts = []\n",
        "    # Para cada registro na lista de registros\n",
        "    for record in records:\n",
        "      # Divide o registro em linhas\n",
        "      lines = record.split(\"\\n\")\n",
        "      # Remove a primeira linha que contém o número do registro\n",
        "      lines.pop(0)\n",
        "      # Junta as linhas restantes em um único texto\n",
        "      text = \" \".join(lines)\n",
        "      # Adiciona o texto à lista de textos\n",
        "      texts.append(text)\n",
        "  # Retorna a lista de textos\n",
        "  return texts\n",
        "\n",
        "\n",
        "# Usa a função criada para ler os documentos, \n",
        "# as consultas e as relevâncias da coleção CISI e \n",
        "# armazená-los em variáveis. Por exemplo:\n",
        "\n",
        "# Lê os documentos da coleção CISI e armazena na variável docs\n",
        "docs = read_cisi_file(\"/content/drive/MyDrive/cisi/CISI.ALL\")\n",
        "# Lê as consultas da coleção CISI e armazena na variável queries\n",
        "queries = read_cisi_file(\"/content/drive/MyDrive/cisi/CISI.QRY\")\n",
        "# Lê as relevâncias da coleção CISI e armazena na variável relevances\n",
        "relevances = read_cisi_file(\"/content/drive/MyDrive/cisi/CISI.REL\")\n",
        "\n",
        "# docs[:2]\n",
        "\n",
        "# Cria uma função para pré-processar os textos dos documentos e das consultas, \n",
        "# removendo pontuação, caracteres especiais, números, stopwords e aplicando stemização usando a \n",
        "# biblioteca nltk\n",
        "\n",
        "\n",
        "# Define uma lista de stopwords em inglês\n",
        "nltk.download('stopwords')\n",
        "stopwords = stopwords.words('english') \n",
        "\n",
        "# Define um stemizador baseado no algoritmo de Porter\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "def preprocess(text):\n",
        "  # Converte o texto para minúsculas\n",
        "  text = text.lower()\n",
        "  # Remove pontuação, caracteres especiais e números usando expressões regulares\n",
        "  text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "  text = re.sub(r\"\\d+\", \"\", text)\n",
        "  # Divide o texto em tokens (palavras) usando espaços em branco como separador\n",
        "  tokens = text.split()\n",
        "  # Remove os tokens que são stopwords usando list comprehension \n",
        "  tokens = [token for token in tokens if token not in stopwords]\n",
        "  # Aplica stemização aos tokens usando list comprehension \n",
        "  tokens = [stemmer.stem(token) for token in tokens]\n",
        "  # Retorna a lista de tokens pré-processados \n",
        "  return tokens\n",
        "\n",
        "# Usa a função para pré-processar os documentos e as consultas da \n",
        "# coleção CISI e armazená-los em novas variáveis. \n",
        "\n",
        "# Pré-processa os documentos da coleção CISI e armazena na variável docs_tokens \n",
        "docs_tokens = [preprocess(doc) for doc in docs]\n",
        "# Pré-processa as consultas da coleção CISI e armazena na variável queries_tokens \n",
        "queries_tokens = [preprocess(query) for query in queries]\n",
        "\n",
        "\n",
        "# Cria um objeto BM25 usando a classe bm25.BM25 da gensim e passa\n",
        "# os documentos tokenizados como argumento\n",
        "bm25_obj = bm25.BM25(docs_tokens)\n",
        "\n",
        "# Calcula média idf um argumento necessario para a funcao get_scores \n",
        "media_idf = sum(bm25_obj.idf.values()) / len(bm25_obj.idf.values())\n",
        "\n",
        "\n",
        "queries_tokens[:3]\n",
        "\n",
        "# Calcula o escore BM25 para cada documento usando o método get_scores do objeto BM25\n",
        "scores = [bm25_obj.get_scores(document = q, average_idf = media_idf) for q in queries_tokens]\n",
        "\n",
        "len(scores)\n",
        "len(scores[0]) == bm25_obj.corpus_size\n",
        "bm25_obj.corpus_size\n",
        "len(queries_tokens) * bm25_obj.corpus_size\n",
        "\n",
        "\n",
        "# Ordenar os documentos de acordo com o escore BM25 e selecionar os mais relevantes para a consulta\n",
        "ranked_docs = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
        "top_docs = ranked_docs[:3]\n"
      ],
      "metadata": {
        "id": "llFy7c98R0on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementração do algoritmo BM25 com avaliação na coleção CISI usando python \"raiz\" :)"
      ],
      "metadata": {
        "id": "sFdLoAIdCz4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "from math import log\n",
        "\n",
        "# Load the document text into a dictionary: The code opens the file \"cisi.all\" \n",
        "# and reads the contents into a Python dictionary called doc_text,\n",
        "# where each document ID is associated with its corresponding text.\n",
        "# this cauded a problem: \"A.I. Mikhailov, and his colleagues\"\n",
        "# I had to change  content.split(\".I\") to content.split(\".I \")\n",
        "# learned using bing chatGPT\n",
        "\n",
        "doc_text = {}\n",
        "with open(\"/content/drive/MyDrive/cisi/CISI.ALL\", \"r\") as f:\n",
        "    content = f.read()\n",
        "    records = content.split(\".I \")\n",
        "    records.pop(0)\n",
        "    lines = f.readlines()\n",
        "    for record in records:\n",
        "      # Divide o registro em linhas\n",
        "      lines = record.split(\"\\n\")\n",
        "      # guarda id\n",
        "      id = int(lines[0])\n",
        "      # print(lines[0])\n",
        "      # print(int(id))\n",
        "      # Remove a primeira linha que contém o número do registro\n",
        "      lines.pop(0)\n",
        "      # Junta as linhas restantes em um único texto\n",
        "      text = \" \".join(lines)\n",
        "      print(text)\n",
        "      # Adiciona o texto à lista de textos\n",
        "      doc_text[id] = text\n",
        "\n",
        "\n",
        "# Load the queries into a list: The code opens the file \"cisi.qry\" and reads the contents\n",
        "# into a list called queries, where each query is represented as a string.\n",
        "\n",
        "queries = []\n",
        "with open(\"/content/drive/MyDrive/cisi/CISI.QRY\", \"r\") as f:\n",
        "    content = f.read()\n",
        "    records = content.split(\".I\")\n",
        "    records.pop(0)\n",
        "    lines = f.readlines()\n",
        "    for record in records:\n",
        "      # Divide o registro em linhas\n",
        "      lines = record.split(\"\\n\")\n",
        "      # guarda id\n",
        "      id = int(lines[0])\n",
        "      # print(lines[0])\n",
        "      # print(int(id))\n",
        "      # Remove a primeira linha que contém o número do registro\n",
        "      lines.pop(0)\n",
        "      # Junta as linhas restantes em um único texto\n",
        "      text = \" \".join(lines)\n",
        "      print(text)\n",
        "      # Adiciona o texto à lista de textos\n",
        "      queries.append(text)\n",
        "\n",
        "\n",
        "# Load the relevance judgments into a dictionary: The code opens the file \"cisi.rel\" \n",
        "# reads the contents into a Python dictionary called relevance_judgments, \n",
        "# each query ID is associated with a list of relevant document IDs.\n",
        "\n",
        "relevance_judgments = {}\n",
        "with open(\"/content/drive/MyDrive/cisi/CISI.REL\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for i in range(0, len(lines)):\n",
        "        query_id, doc_id, _, _ = lines[i].split()\n",
        "        if query_id not in relevance_judgments:\n",
        "            relevance_judgments[query_id] = []\n",
        "        relevance_judgments[query_id].append(doc_id)\n",
        "\n",
        "\n",
        "# Define the BM25 ranking function\n",
        "# the BM25 ranking function: The code defines a function called bm25 that takes \n",
        "# as input and calculates the BM25 score for each document in the database. \n",
        "# and b parameters are set to default values of 1.2 and 0.75, respectively.\n",
        "\n",
        "def bm25(query, doc_text, k1=1.2, b=0.75):\n",
        "    query_terms = query.lower().split()\n",
        "    scores = Counter()\n",
        "    for doc_id, doc_text in doc_text.items():\n",
        "        doc_terms = doc_text.lower().split()\n",
        "        doc_length = len(doc_terms)\n",
        "        doc_term_counts = Counter(doc_terms)\n",
        "        for term in query_terms:\n",
        "            if term in doc_term_counts:\n",
        "                idf = log(len(doc_text) / doc_term_counts[term])\n",
        "                tf = doc_term_counts[term]\n",
        "                score = idf * ((k1 + 1) * tf) / (k1 * ((1 - b) + b * (doc_length / len(doc_terms))) + tf)\n",
        "                scores[doc_id] += score\n",
        "    return scores\n",
        "\n",
        "\n",
        "# Calculates the bm25 scores for each document for each query.\n",
        "# stores in a list results with [N queries [N docs]]\n",
        "results = [bm25(query=q, doc_text=doc_text) for q in queries]\n",
        "\n",
        "len(results )\n",
        "len(results[0])\n",
        "\n",
        "# bm25(query=queries[0], doc_text=doc_text)\n",
        "\n",
        "# Define the evaluation function\n",
        "# The code defines a function called evaluate the results of the ranking \n",
        "# (as a dictionary of document scores for each query) \n",
        "# relevance judgments as input, and calculates the precision, recall, \n",
        "# and F1 score of the ranking.\n",
        "\n",
        "def evaluate(results, relevance_judgments):\n",
        "    num_queries = len(relevance_judgments)\n",
        "    precision = 0.0\n",
        "    recall = 0.0\n",
        "    f1 = 0.0\n",
        "    for query_id, docs in relevance_judgments.items():\n",
        "        retrieved_docs = [doc_id for doc_id, _ in results[query_id]]\n",
        "        relevant_docs = set(docs)\n",
        "        retrieved_and_relevant = relevant_docs.intersection(retrieved_docs)\n",
        "        precision += len(retrieved_and_relevant) / len(retrieved_docs)\n",
        "        recall += len(retrieved_and_relevant) / len(relevant_docs)\n",
        "    precision /= num_queries\n",
        "    recall /= num_queries\n",
        "    try:\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "    except ZeroDivisionError:\n",
        "        f1 = 0\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Perform the BM25 ranking for each query and evaluate the results\n",
        "# the BM25 ranking and evaluation: The code uses a loop to iterate over each \n",
        "#n the queries list, and applies the bm25 function to calculate the document \n",
        "#for the query. The results are stored in a dictionary called results, \n",
        "#ach query ID is associated with a list of the top 100 documents ranked by BM25 score. \n",
        "#\n",
        "#, the evaluate function is called to calculate the precision, recall, \n",
        "#score of the ranking, and the results are printed to the console.\n",
        "results = {}\n",
        "for i, query in enumerate(queries):\n",
        "    query_id = str(i+1)\n",
        "    scores = bm25(query, doc_text=doc_text)\n",
        "    results[query_id] = scores.most_common(100)\n",
        "\n",
        "precision, recall, f1 = evaluate(results, relevance_judgments)\n",
        "\n"
      ],
      "metadata": {
        "id": "aMRKWgSsQEr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "from math import log\n",
        "\n",
        "# Load the document text into a dictionary: The code opens the file \"cisi.all\" \n",
        "# and reads the contents into a Python dictionary called doc_text,\n",
        "# where each document ID is associated with its corresponding text.\n",
        "# this cauded a problem: \"A.I. Mikhailov, and his colleagues\"\n",
        "# I had to change  content.split(\".I\") to content.split(\".I \")\n",
        "# learned using bing chatGPT\n",
        "\n",
        "doc_text = {}\n",
        "with open(\"/content/drive/MyDrive/cisi/CISI.ALL\", \"r\") as f:\n",
        "    content = f.read()\n",
        "    records = content.split(\".I \")\n",
        "    records.pop(0)\n",
        "    lines = f.readlines()\n",
        "    for record in records:\n",
        "      # Divide o registro em linhas\n",
        "      lines = record.split(\"\\n\")\n",
        "      # guarda id\n",
        "      id = int(lines[0])\n",
        "      # print(lines[0])\n",
        "      # print(int(id))\n",
        "      # Remove a primeira linha que contém o número do registro\n",
        "      lines.pop(0)\n",
        "      # Junta as linhas restantes em um único texto\n",
        "      text = \" \".join(lines)\n",
        "      print(text)\n",
        "      # Adiciona o texto à lista de textos\n",
        "      doc_text[id] = text\n",
        "\n",
        "\n",
        "# Load the queries into a list: The code opens the file \"cisi.qry\" and reads the contents\n",
        "# into a list called queries, where each query is represented as a string.\n",
        "\n",
        "queries = []\n",
        "with open(\"/content/drive/MyDrive/cisi/CISI.QRY\", \"r\") as f:\n",
        "    content = f.read()\n",
        "    records = content.split(\".I\")\n",
        "    records.pop(0)\n",
        "    lines = f.readlines()\n",
        "    for record in records:\n",
        "      # Divide o registro em linhas\n",
        "      lines = record.split(\"\\n\")\n",
        "      # guarda id\n",
        "      id = int(lines[0])\n",
        "      # print(lines[0])\n",
        "      # print(int(id))\n",
        "      # Remove a primeira linha que contém o número do registro\n",
        "      lines.pop(0)\n",
        "      # Junta as linhas restantes em um único texto\n",
        "      text = \" \".join(lines)\n",
        "      print(text)\n",
        "      # Adiciona o texto à lista de textos\n",
        "      queries.append(text)\n",
        "\n",
        "\n",
        "# Load the relevance judgments into a dictionary: The code opens the file \"cisi.rel\" \n",
        "# reads the contents into a Python dictionary called relevance_judgments, \n",
        "# each query ID is associated with a list of relevant document IDs.\n",
        "\n",
        "relevance_judgments = {}\n",
        "with open(\"/content/drive/MyDrive/cisi/CISI.REL\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for i in range(0, len(lines)):\n",
        "        query_id, doc_id, _, _ = lines[i].split()\n",
        "        if query_id not in relevance_judgments:\n",
        "            relevance_judgments[query_id] = []\n",
        "        relevance_judgments[query_id].append(str(doc_id))\n",
        "\n",
        "\n",
        "# Define the BM25 ranking function\n",
        "# the BM25 ranking function: The code defines a function called bm25 that takes \n",
        "# as input and calculates the BM25 score for each document in the database. \n",
        "# and b parameters are set to default values of 1.2 and 0.75, respectively.\n",
        "\n",
        "def bm25(query, doc_text, k1=1.2, b=0.75):\n",
        "    query_terms = query.lower().split()\n",
        "    scores = Counter()\n",
        "    for doc_id, doc_text in doc_text.items():\n",
        "        doc_terms = doc_text.lower().split()\n",
        "        doc_length = len(doc_terms)\n",
        "        doc_term_counts = Counter(doc_terms)\n",
        "        for term in query_terms:\n",
        "            if term in doc_term_counts:\n",
        "                idf = log(len(doc_text) / doc_term_counts[term])\n",
        "                tf = doc_term_counts[term]\n",
        "                score = idf * ((k1 + 1) * tf) / (k1 * ((1 - b) + b * (doc_length / len(doc_terms))) + tf)\n",
        "                scores[doc_id] += score\n",
        "    return scores\n",
        "\n",
        "\n",
        "# Calculates the bm25 scores for each document for each query.\n",
        "# stores in a list results with [N queries [N docs]]\n",
        "results = [bm25(query=q, doc_text=doc_text) for q in queries]\n",
        "\n",
        "len(results )\n",
        "len(results[0])\n",
        "\n",
        "# bm25(query=queries[0], doc_text=doc_text)\n",
        "\n",
        "# Define the evaluation function\n",
        "# The code defines a function called evaluate the results of the ranking \n",
        "# (as a dictionary of document scores for each query) \n",
        "# relevance judgments as input, and calculates the precision, recall, \n",
        "# and F1 score of the ranking.\n",
        "\n",
        "def evaluate(results, relevance_judgments):\n",
        "    num_queries = len(relevance_judgments)\n",
        "    precision = 0.0\n",
        "    recall = 0.0\n",
        "    f1 = 0.0\n",
        "    for query_id, docs in relevance_judgments.items():\n",
        "        retrieved_docs = [doc_id for doc_id, _ in results[query_id]]\n",
        "        relevant_docs = set(docs)\n",
        "        retrieved_and_relevant = relevant_docs.intersection(retrieved_docs)\n",
        "        precision += len(retrieved_and_relevant) / len(retrieved_docs)\n",
        "        recall += len(retrieved_and_relevant) / len(relevant_docs)\n",
        "    precision /= num_queries\n",
        "    recall /= num_queries\n",
        "    try:\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "    except ZeroDivisionError:\n",
        "        f1 = 0\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Perform the BM25 ranking for each query and evaluate the results\n",
        "# the BM25 ranking and evaluation: The code uses a loop to iterate over each \n",
        "#n the queries list, and applies the bm25 function to calculate the document \n",
        "#for the query. The results are stored in a dictionary called results, \n",
        "#ach query ID is associated with a list of the top 100 documents ranked by BM25 score. \n",
        "#\n",
        "#, the evaluate function is called to calculate the precision, recall, \n",
        "#score of the ranking, and the results are printed to the console.\n",
        "results = {}\n",
        "for i, query in enumerate(queries):\n",
        "    query_id = str(i+1)\n",
        "    scores = bm25(query, doc_text=doc_text)\n",
        "    results[query_id] = scores.most_common(100)\n",
        "\n",
        "\n",
        "precision, recall, f1 = evaluate(results, relevance_judgments)\n",
        "\n"
      ],
      "metadata": {
        "id": "7_BRqTOcczNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "from math import log\n",
        "\n",
        "# Load the document text into a dictionary: The code opens the file \"cisi.all\" \n",
        "# and reads the contents into a Python dictionary called doc_text,\n",
        "# where each document ID is associated with its corresponding text.\n",
        "# this cauded a problem: \"A.I. Mikhailov, and his colleagues\"\n",
        "# I had to change  content.split(\".I\") to content.split(\".I \")\n",
        "# learned using bing chatGPT\n",
        "\n",
        "doc_text = {}\n",
        "with open(\"/content/drive/MyDrive/cisi/CISI.ALL\", \"r\") as f:\n",
        "    content = f.read()\n",
        "    records = content.split(\".I \")\n",
        "    records.pop(0)\n",
        "    lines = f.readlines()\n",
        "    for record in records:\n",
        "      # Divide o registro em linhas\n",
        "      lines = record.split(\"\\n\")\n",
        "      # guarda id\n",
        "      id = lines[0].strip()\n",
        "      # print(lines[0])\n",
        "      # print(int(id))\n",
        "      # Remove a primeira linha que contém o número do registro\n",
        "      lines.pop(0)\n",
        "      # Junta as linhas restantes em um único texto\n",
        "      text = \" \".join(lines)\n",
        "      print(text)\n",
        "      # Adiciona o texto à lista de textos\n",
        "      doc_text[id] = text\n",
        "\n",
        "\n",
        "# Load the queries into a list: The code opens the file \"cisi.qry\" and reads the contents\n",
        "# into a list called queries, where each query is represented as a string.\n",
        "\n",
        "queries = []\n",
        "with open(\"/content/drive/MyDrive/cisi/CISI.QRY\", \"r\") as f:\n",
        "    content = f.read()\n",
        "    records = content.split(\".I\")\n",
        "    records.pop(0)\n",
        "    lines = f.readlines()\n",
        "    for record in records:\n",
        "      # Divide o registro em linhas\n",
        "      lines = record.split(\"\\n\")\n",
        "      # guarda id\n",
        "      id = lines[0].strip()\n",
        "      # print(lines[0])\n",
        "      # print(int(id))\n",
        "      # Remove a primeira linha que contém o número do registro\n",
        "      lines.pop(0)\n",
        "      # Junta as linhas restantes em um único texto\n",
        "      text = \" \".join(lines)\n",
        "      print(text)\n",
        "      # Adiciona o texto à lista de textos\n",
        "      queries.append(text)\n",
        "\n",
        "\n",
        "# Load the relevance judgments into a dictionary: The code opens the file \"cisi.rel\" \n",
        "# reads the contents into a Python dictionary called relevance_judgments, \n",
        "# each query ID is associated with a list of relevant document IDs.\n",
        "\n",
        "relevance_judgments = {}\n",
        "with open(\"/content/drive/MyDrive/cisi/CISI.REL\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for i in range(0, len(lines)):\n",
        "        query_id, doc_id, _, _ = lines[i].split()\n",
        "        if query_id not in relevance_judgments:\n",
        "            relevance_judgments[query_id] = []\n",
        "        relevance_judgments[query_id].append(doc_id)\n",
        "\n",
        "\n",
        "# Define the BM25 ranking function\n",
        "# the BM25 ranking function: The code defines a function called bm25 that takes \n",
        "# as input and calculates the BM25 score for each document in the database. \n",
        "# and b parameters are set to default values of 1.2 and 0.75, respectively.\n",
        "\n",
        "def bm25(query, doc_text, k1=1.2, b=0.75):\n",
        "    query_terms = query.lower().split()\n",
        "    scores = Counter()\n",
        "    for doc_id, doc_text in doc_text.items():\n",
        "        doc_terms = doc_text.lower().split()\n",
        "        doc_length = len(doc_terms)\n",
        "        doc_term_counts = Counter(doc_terms)\n",
        "        for term in query_terms:\n",
        "            if term in doc_term_counts:\n",
        "                idf = log(len(doc_text) / doc_term_counts[term])\n",
        "                tf = doc_term_counts[term]\n",
        "                score = idf * ((k1 + 1) * tf) / (k1 * ((1 - b) + b * (doc_length / len(doc_terms))) + tf)\n",
        "                scores[doc_id] += score\n",
        "    return scores\n",
        "\n",
        "\n",
        "# Calculates the bm25 scores for each document for each query.\n",
        "# stores in a list results with [N queries [N docs]]\n",
        "# results = [bm25(query=q, doc_text=doc_text) for q in queries]\n",
        "#len(results )\n",
        "#len(results[0])\n",
        "\n",
        "\n",
        "\n",
        "# bm25(query=queries[0], doc_text=doc_text)\n",
        "\n",
        "# Define the evaluation function\n",
        "# The code defines a function called evaluate the results of the ranking \n",
        "# (as a dictionary of document scores for each query) \n",
        "# relevance judgments as input, and calculates the precision, recall, \n",
        "# and F1 score of the ranking.\n",
        "\n",
        "def evaluate(results, relevance_judgments):\n",
        "    num_queries = len(relevance_judgments)\n",
        "    precision = 0.0\n",
        "    recall = 0.0\n",
        "    f1 = 0.0\n",
        "    for query_id, docs in relevance_judgments.items():\n",
        "        retrieved_docs = [str(doc_id) for doc_id, _ in results[query_id]]\n",
        "        relevant_docs = set(docs)\n",
        "        retrieved_and_relevant = relevant_docs.intersection(retrieved_docs)\n",
        "        precision += len(retrieved_and_relevant) / len(retrieved_docs)\n",
        "        recall += len(retrieved_and_relevant) / len(relevant_docs)\n",
        "    precision /= num_queries\n",
        "    recall /= num_queries\n",
        "    try:\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "    except ZeroDivisionError:\n",
        "        f1 = 0\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Perform the BM25 ranking for each query and evaluate the results\n",
        "# Loop over each queries list, and applies the bm25 function to calculate the document scores \n",
        "# for the query. \n",
        "# The results are stored in a dictionary called results. Each query ID is associated\n",
        "# with a list of the top 100 documents ranked by BM25 score. \n",
        "# Then the evaluate function is called to calculate the precision, recall, \n",
        "# score of the ranking, and the results are printed to the console.\n",
        "\n",
        "results = {}\n",
        "for i, query in enumerate(queries):\n",
        "    query_id = str(i+1)\n",
        "    scores = bm25(query, doc_text=doc_text)\n",
        "    results[query_id] = scores.most_common(100)\n",
        "\n",
        "precision, recall, f1 = evaluate(results, relevance_judgments)\n",
        "\n",
        "print(\"Precision: {:.2f}%\".format(precision * 100))\n",
        "print(\"Recall: {:.2f}%\".format(recall * 100))\n",
        "print(\"F1: {:.2f}%\".format(f1 * 100))   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "987caf8e-e813-462c-b527-d678d857ede5",
        "id": "GptABltic0x1"
      },
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 7.66%\n",
            "Recall: 20.41%\n",
            "F1: 11.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solução usando gensin e nltk"
      ],
      "metadata": {
        "id": "KhWAbm-kgCLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Sim, eu posso te mostrar um código em Python que usa o algoritmo BM25 para classificar documentos. Este código é baseado no exemplo do link¹ que eu mencionei antes. Você pode adaptá-lo para os seus dados CISI.\n",
        "\n",
        "# Importar as bibliotecas necessárias\n",
        "import nltk\n",
        "from gensim.summarization import bm25\n",
        "\n",
        "# Carregar os dados CISI e dividi-los em documentos e consultas\n",
        "# Aqui eu estou usando um exemplo simples com 3 documentos e 1 consulta\n",
        "docs = [\"O Brasil é um país da América do Sul.\",\n",
        "        \"A França é um país da Europa Ocidental.\",\n",
        "        \"A China é um país da Ásia Oriental.\"]\n",
        "query = \"Qual país fica na Europa?\"\n",
        "\n",
        "# Pré-processar os documentos e consultas usando nltk para tokenizar, remover stopwords e aplicar stemming\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "stemmer = nltk.stem.RSLPStemmer()\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token not in stopwords and token.isalpha()]\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "docs_tokens = [preprocess(doc) for doc in docs]\n",
        "query_tokens = preprocess(query)\n",
        "\n",
        "# Criar um objeto BM25 usando a classe bm25.BM25 da gensim e passar os documentos tokenizados como argumento\n",
        "bm25_obj = bm25.BM25(docs_tokens)\n",
        "\n",
        "# Para cada consulta, calcular o escore BM25 para cada documento usando o método get_scores do objeto BM25\n",
        "scores = bm25_obj.get_scores(query_tokens)\n",
        "\n",
        "# Ordenar os documentos de acordo com o escore BM25 e selecionar os mais relevantes para a consulta\n",
        "ranked_docs = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
        "top_docs = ranked_docs[:3]\n",
        "\n",
        "# Mostrar os resultados\n",
        "print(f\"Consulta: {query}\")\n",
        "print(f\"Documentos mais relevantes:\")\n",
        "for doc, score in top_docs:\n",
        "    print(f\"- {doc} (escore: {score:.2f})\")\n",
        "\n",
        "\n",
        "Source: Conversation with Bing, 20/02/2023(1) BM25 | Build your Own NLP Based Search Engine Using BM25 - Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25/ Accessed 20/02/2023.\n",
        "(2) How to use gensim BM25 ranking in python - Stack Overflow. https://stackoverflow.com/questions/40966014/how-to-use-gensim-bm25-ranking-in-python Accessed 20/02/2023.\n",
        "(3) GitHub - nhirakawa/BM25: A Python implementation of the BM25 ranking .... https://github.com/nhirakawa/BM25 Accessed 20/02/2023."
      ],
      "metadata": {
        "id": "N4G7L5DBjicX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here is a Python code that implements a information search retrieval using BM25 \n",
        "#and evaluates the system on CISI collection. I have added comments to explain the details.\n",
        "\n",
        "# Importing libraries\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from gensim import corpora, models, similarities\n",
        "\n",
        "# Defining a function to read and tokenize documents from a file\n",
        "def read_docs(file):\n",
        "    docs = []\n",
        "    with open(file, \"r\") as f:\n",
        "        for line in f:\n",
        "            # Each document starts with .I followed by an id number\n",
        "            if line.startswith(\".I\"):\n",
        "                doc = \"\"\n",
        "                doc_id = line.strip()\n",
        "            # Each document ends with .X followed by references\n",
        "            elif line.startswith(\".X\"):\n",
        "                docs.append((doc_id, doc.split()))\n",
        "            else:\n",
        "                # Removing punctuation and numbers from the text\n",
        "                line = re.sub(\"[^a-zA-Z\\s]\", \"\", line)\n",
        "                doc += line.lower()\n",
        "    return docs\n",
        "\n",
        "# Defining a function to read and tokenize queries from a file\n",
        "def read_queries(file):\n",
        "    queries = []\n",
        "    with open(file, \"r\") as f:\n",
        "        for line in f:\n",
        "            # Each query starts with .I followed by an id number\n",
        "            if line.startswith(\".I\"):\n",
        "                query = \"\"\n",
        "                query_id = line.strip()\n",
        "            # Each query ends with .W followed by text\n",
        "            elif line.startswith(\".W\"):\n",
        "                query += f.readline().lower()\n",
        "                queries.append((query_id, query.split()))\n",
        "    return queries\n",
        "\n",
        "# Defining a function to read relevance judgments from a file\n",
        "def read_relevance(file):\n",
        "    relevance = {}\n",
        "    with open(file, \"r\") as f:\n",
        "        for line in f:\n",
        "            # Each line contains a query id, a document id and a relevance score (1 or 0)\n",
        "            qid, did, rel = line.split()\n",
        "            # Storing the relevance judgments in a dictionary of sets\n",
        "            if qid not in relevance:\n",
        "                relevance[qid] = set()\n",
        "            if rel == \"1\":\n",
        "                relevance[qid].add(did)\n",
        "    return relevance\n",
        "\n",
        "\n",
        "\n",
        "# Lê os documentos da coleção CISI e armazena na variável docs\n",
        "docs = read_cisi_file(\"/content/drive/MyDrive/cisi/CISI.ALL\")\n",
        "# Lê as consultas da coleção CISI e armazena na variável queries\n",
        "queries = read_cisi_file(\"/content/drive/MyDrive/cisi/CISI.QRY\")\n",
        "# Lê as relevâncias da coleção CISI e armazena na variável relevances\n",
        "relevances = read_cisi_file(\"/content/drive/MyDrive/cisi/CISI.REL\")\n",
        "\n",
        "\n",
        "# Reading and tokenizing the documents from cisi.all file\n",
        "docs = read_docs(\"/content/drive/MyDrive/cisi/CISI.ALL\")\n",
        "\n",
        "# Creating a dictionary of terms from the documents\n",
        "dictionary = corpora.Dictionary(doc[1] for doc in docs)\n",
        "\n",
        "# Creating a corpus of bag-of-words vectors from the documents\n",
        "corpus = [dictionary.doc2bow(doc[1]) for doc in docs]\n",
        "\n",
        "# Initializing a BM25 model from gensim library using the corpus and dictionary \n",
        "bm25_model = bm25.BM25Mode(corpus=corpus, id2word=dictionary)\n",
        "\n",
        "\n",
        "\n",
        "# Reading and tokenizing the queries from cisi.que file \n",
        "queries = read_queries(\"cisi.que\")\n",
        "\n",
        "# Reading the relevance judgments from cisi.rel file \n",
        "relevance = read_relevance(\"cisi.rel\")\n",
        "\n",
        "# Defining some evaluation metrics \n",
        "precision_at_10 = 0 # Precision at rank 10 \n",
        "average_precision = 0 # Mean average precision \n",
        "recall_at_50 = 0 # Recall at rank 50 \n",
        "num_queries = len(queries) # Number of queries \n",
        "\n",
        "# Looping through each query \n",
        "for query in queries:\n",
        "    # Getting the query id and terms \n",
        "    qid, qterms = query \n",
        "    # Converting the query terms to bag-of-words vector \n",
        "    qvec = dictionary.doc2bow(qterms) \n",
        "    # Getting the BM25 scores for each document in the corpus using the query vector \n",
        "    scores = bm25_model.get_scores(qvec) \n",
        "    # Sorting the scores in descending order and getting the top 50 documents ids  \n",
        "    ranked_docs_ids= sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:50]  \n",
        "    ranked_docs_ids= [docs[i][0] for i in ranked_docs_ids]  \n",
        "   \n",
        "   ### Evaluating precision at rank 10 ###\n",
        "   relevant_at_10= set(ranked_docs_ids[:10]) & relevance[qid]  \n",
        "   precision_at_10+= len(relevant_at_10) / 10  \n",
        "\n",
        "   ### Evaluating mean average precision ###\n",
        "   num_relevant= 0  \n",
        "   sum_precision= 0  \n",
        "   for i, didin enumerate(ranked_docs_ids):  \n",
        "       if didinrelevance[qid]:  \n",
        "           num_relevant+= 1  \n",
        "           sum_precision+= num_relevant / (i + 1)  \n",
        "   average_precision+= sum_precision / len(relevance[qid])  \n",
        "\n",
        "   ### Evaluating recall at rank 50 ###\n",
        "   relevant_at_50= set(ranked_docs_ids) & relevance[qid]\n",
        "\n",
        "Origem: conversa com o Bing, 16/02/2023(1) How to use gensim BM25 ranking in python - Stack Overflow. https://stackoverflow.com/questions/40966014/how-to-use-gensim-bm25-ranking-in-python Acessado 16/02/2023.\n",
        "(2) BM25 | Build your Own NLP Based Search Engine Using BM25 - Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25/ Acessado 16/02/2023.\n",
        "(3) svm - Implementation of Okapi BM25 in python - Stack Overflow. https://stackoverflow.com/questions/61877065/implementation-of-okapi-bm25-in-python Acessado 16/02/2023.\n",
        "(4) Glasgow IDOM - CISI collection. http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/ Acessado 16/02/2023.\n",
        "(5) Average precisions for CISI collection: 1000 documents are added in 100 .... https://www.researchgate.net/figure/Average-precisions-for-CISI-collection-1000-documents-are-added-in-100-groups-of-10_fig2_228961662 Acessado 16/02/2023.\n",
        "(6) Comparison of normalized error for the CISI collection: 1000 documents .... https://www.researchgate.net/figure/Comparison-of-normalized-error-for-the-CISI-collection-1000-documents-are-added-in-100_fig7_228961662 Acessado 16/02/2023.\n",
        "(7) How to use gensim BM25 ranking in python - Stack Overflow. https://stackoverflow.com/questions/40966014/how-to-use-gensim-bm25-ranking-in-python Acessado 16/02/2023.\n",
        "(8) rank-bm25 · PyPI. https://pypi.org/project/rank-bm25/ Acessado 16/02/2023.\n",
        "(9) BM25 | Build your Own NLP Based Search Engine Using BM25 - Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25/ Acessado 16/02/2023."
      ],
      "metadata": {
        "id": "ftjH6imvT5re",
        "outputId": "a8427430-7e71-4fa7-958e-2871606e6737",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-137-91cdb8958614>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbm25_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbm25\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBM25Mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'BM25Okapi' object has no attribute 'BM25Mode'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BM25 is a ranking function that estimates the relevance of documents to a given search query based on the frequency and importance of the query terms in the documents². It is a bag-of-words retrieval function, which means it does not consider the order or proximity of the query terms within the document². It is based on the probabilistic retrieval framework developed by Stephen E. Robertson, Karen Spärck Jones, and others²⁵.\n",
        "\n",
        "Origem: conversa com o Bing, 16/02/2023(1) Okapi BM25 - Wikipedia. https://en.wikipedia.org/wiki/Okapi_BM25 Acessado 16/02/2023.\n",
        "(2) How does BM25 work? - Quora. https://www.quora.com/How-does-BM25-work Acessado 16/02/2023.\n",
        "(3) BM25 | Build your Own NLP Based Search Engine Using BM25 - Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25/#:~:text=BM25%20is%20a%20simple%20Python%20package%20and%20can,occurrences%20of%20the%20search%20term%20in%20our%20tweet Acessado 16/02/2023.\n",
        "(4) BM25 | Build your Own NLP Based Search Engine Using BM25 - Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25/ Acessado 16/02/2023.\n",
        "(5) Practical BM25 - Part 2: The BM25 Algorithm and its Variables. https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables Acessado 16/02/2023."
      ],
      "metadata": {
        "id": "7GUt5H7sUKKg"
      }
    }
  ]
}