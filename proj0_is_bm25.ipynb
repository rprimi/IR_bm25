{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOB6OrArRs4tLeIzm09wU/T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rprimi/IR_bm25/blob/main/proj0_is_bm25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Building a Simple Information Retrieval System using BM25\n",
        "\n",
        "## Description\n",
        "\n",
        "Exercise for special student selection for the Course \"Deep Learning applied to search systems\", FEEC-Unicamp.\n",
        "\n",
        "**Objectives:** \n",
        "- Create a basic system of Information Retrieval (IR) using the BM25 ranking algorithm in Python.\n",
        "\n",
        "**Deliverables:**\n",
        "- A github repository\n",
        "- Google Colab notebooks and associated functions and libraries if any, with the code for the IR system\n",
        "- A brief report describing the implementation details, results, how to test the IR system and how chatGPT helped you with the project.\n",
        "- Evaluation of the model in the CISI collection\n",
        "http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/\n",
        "\n",
        "**Grading:**  \n",
        "The project will be evaluated based on the following criteria:\n",
        "- Completion of the project deliverables (partial is OK to submit)\n",
        "- Implementation accuracy\n",
        "- Quality of the report\n",
        "- Code readability and commenting\n",
        "- How chatGPT was used\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZfrttcD9tn85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning about BM25\n",
        "\n",
        "BM25 is a ranking function used by search engines to estimate the relevance of documents for a given search query¹². It is based on the probabilistic information retrieval model developed in the 1970s and 1980s by Stephen E. Robertson, Karen Spärck Jones and others².\n",
        "\n",
        "BM25 is a function that scores a set of documents based on the query terms that appear in each document, regardless of how close the terms are within the document². It is a family of scoring functions with slightly different components and parameters. One of the most prominent instances of the function is the following²:\n",
        "\n",
        "$$\n",
        "\\mathrm{score}(D,Q) = \\sum_{i=1}^{n} \\mathrm{IDF}(q_i) \\cdot \\frac{f(q_i,D) \\cdot (k_1 + 1)}{f(q_i,D) + k_1 \\cdot (1 - b + b \\cdot D / \\mathrm{avgdl})}\n",
        "$$\n",
        "\n",
        "\n",
        "Where:\n",
        "- $score(D,Q)$ is the BM25 score of document $D$ for query $Q$\n",
        "- $\\sum_{i=1}^{n}$ is the sum over all query terms $q_i$ that appear in document $D$\n",
        "- The $IDF(q_i)$ is given by:\n",
        "\n",
        "$$\\mathrm{IDF}(q_i) = \\log{_e} \\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}$$\n",
        "- where:\n",
        "  - $N$ is the total number of documents in the collection\n",
        "  - $n(q_i)$ is the number of documents that contain the term $q_i$\n",
        "  - The smaller the value of $n(q_i)$, the greater the value of $IDF(q_i)$ and the greater the contribution of the term to the final score. The IDF increases as the term becomes rarer in the collection, and decreases as it becomes more common. \"The IDF component of our formula measures how often a term occurs in all of the documents and “penalizes” terms that are common\"\n",
        "- $f(q_i,D)$ is the frequency of the term $q_i$ in the document $D$, that is, how many times it appears in the text. The higher this value, the more relevant the document is to the term.\n",
        "- $D$ is the length of the document D in words. $\\mathrm{avgdl}$ is the average length of the documents in the collection. The length of the documento is divided by the average doc length in the denominator: $D / \\mathrm{avgdl}$ .\"The way to think about this is that the more terms in the document — at least ones not matching the query — the lower the score for the document\"²\n",
        "- $b$ controls how much the score is affected by document length relative to the average length of documents in the collection. Larger values of $b$ favor shorter documents, while smaller values favor longer documents. The commonly used default value is $b = 0.75$. \"if b is bigger, the effects of the length of the document compared to the average length are more amplified.\"\n",
        "\n",
        "- $(k_1 + 1)$ and $(k_1 * (1 - b + b * |D| / avgdl))$ are two components that adjust for the influence of term frequency on the final score. They depend on the free parameters:\n",
        "- $k_1$ controls how much the score increases based on term frequency. Larger values of $k_1$ mean that differences in term frequency have more impact on the score, while smaller values mean that differences have less impact. The commonly used default value is $k_1 = 2.0$.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "These elements together form a ranking function that attempts to estimate how relevant a document is to a query based on the terms they share.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Espero que isso tenha esclarecido os elementos da fórmula BM25.\n",
        "\n",
        "Origem: conversa com o Bing, 16/02/2023(1) Okapi BM25 - Wikipedia. https://en.wikipedia.org/wiki/Okapi_BM25 Acessado 16/02/2023.\n",
        "(2) Practical BM25 - Part 2: The BM25 Algorithm and its Variables. https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables Acessado 16/02/2023.\n",
        "(3) Document similarities with BM25 algorithm - MATLAB bm25Similarity. https://www.mathworks.com/help/textanalytics/ref/bm25similarity.html Acessado 16/02/2023.\n",
        "\n",
        "\n",
        "Origem: conversa com o Bing, 16/02/2023(1) Okapi BM25 - Wikipedia. https://en.wikipedia.org/wiki/Okapi_BM25 Acessado 16/02/2023.\n",
        "(2) Okapi BM25 - Wikipedia. https://en.wikipedia.org/wiki/Okapi_BM25 Acessado 16/02/2023.\n",
        "(3) python-bm25 | Python implementation of BM25 function | Search Engine .... https://kandi.openweaver.com/python/fanta-mnix/python-bm25#:~:text=python-bm25%20is%20a%20Python%20library%20typically%20used%20in,Python%20implementation%20of%20BM25%20function%20for%20document%20retrieval Acessado 16/02/2023.\n",
        "(4) BM25 using Python Gensim Package | Search Engine. https://iamgeekydude.com/2022/12/25/bm25-using-python-gensim-package-search-engine-nlp/ Acessado 16/02/2023.\n",
        "(5) GitHub - nhirakawa/BM25: A Python implementation of the BM25 ranking .... https://github.com/nhirakawa/BM25 Acessado 16/02/2023.\n",
        "(6) GitHub - fanta-mnix/python-bm25: Python implementation of BM25 function .... https://github.com/fanta-mnix/python-bm25 Acessado 16/02/2023.\n",
        "(7) GitHub - xianchen2/Text_Retrieval_BM25: Python implementation of the .... https://github.com/xianchen2/Text_Retrieval_BM25 Acessado 16/02/2023.\n",
        "(8) Machine Learning Approach for Improved BM25 Retrieval. https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/LearningBM25MSRTechReport.pdf Acessado 16/02/2023.\n",
        "(9) Information retrieval evaluation of precision, recall, f-score, AP, MAP .... https://stackoverflow.com/questions/40457331/information-retrieval-evaluation-of-precision-recall-f-score-ap-map-in-pytho Acessado 16/02/2023.\n",
        "(10) Information Retrieval with document Re-ranking with BERT and BM25. https://medium.com/@papai143/information-retrieval-with-document-re-ranking-with-bert-and-bm25-7c29d738df73 Acessado 16/02/2023.\n",
        "(11) python-bm25 | Python implementation of BM25 function | Search Engine .... https://kandi.openweaver.com/python/fanta-mnix/python-bm25 Acessado 16/02/2023."
      ],
      "metadata": {
        "id": "Xym-jV_QvbwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Claro, eu posso explicar em detalhes o que é cada elemento da fórmula acima score(D,Q) usando fórmulas em LaTeX.\n",
        "\n",
        "A fórmula score(D,Q) calcula a relevância de um documento D para uma consulta Q, usando uma função de ranqueamento baseada no modelo probabilístico²³. Ela é composta pelos seguintes elementos:\n",
        "\n",
        "- $\\mathrm{IDF}(q_i)$ é o inverso da frequência do documento do termo $q_i$, que é uma medida de quão informativo ou discriminativo é o termo para diferenciar os documentos relevantes dos não relevantes. Ele é calculado como:\n",
        "\n",
        "\n",
        "$$\n",
        "\\mathrm{IDF}(q_i) = \\log \\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}\n",
        "$$\n",
        "\n",
        "\n",
        "onde N é o número total de documentos na coleção e $n(q_i)$ é o número de documentos que contêm o termo $q_i$. O IDF aumenta à medida que o termo se torna mais raro na coleção, e diminui à medida que se torna mais comum.\n",
        "\n",
        "- $f(q_i,D)$ é a frequência do termo $q_i$ no documento D, que é uma medida de quão importante ou relevante é o termo para o conteúdo do documento. Ela pode ser simplesmente a contagem do número de vezes que o termo aparece no documento, ou alguma outra função que leve em conta a posição, a proximidade ou a ponderação dos termos.\n",
        "\n",
        "- $k_1$ e b são parâmetros ajustáveis que controlam a influência da frequência do termo e do comprimento do documento na pontuação final. Eles podem variar dependendo da coleção, da consulta ou da tarefa de recuperação. Valores típicos são $k_1 \\in [1.2, 2]$ e $b = 0.75$².\n",
        "\n",
        "- $(k_1 + 1)$ e $(k_1 \\cdot (1 - b + b \\cdot |D| / \\mathrm{avgdl}))$ são fatores de normalização que evitam que documentos muito longos ou muito curtos tenham vantagem ou desvantagem na pontuação final. Eles levam em conta o comprimento do documento D ($|D|$), comparando-o com o comprimento médio dos documentos na coleção ($\\mathrm{avgdl}$). O parâmetro b controla o grau de normalização: quanto mais próximo de 1, mais sensível ao comprimento; quanto mais próximo de 0, menos sensível.\n",
        "\n",
        "A fórmula score(D,Q) soma os produtos dos elementos acima para cada termo da consulta Q, dando assim uma pontuação global para o documento D em relação à consulta Q. Quanto maior a pontuação, maior a relevância estimada.\n",
        "\n",
        "Espero que isso tenha esclarecido os elementos da fórmula BM25.\n",
        "\n",
        "Origem: conversa com o Bing, 16/02/2023(1) (PDF) A Tutorial on the BM25F Model - ResearchGate. https://www.researchgate.net/publication/308991534_A_Tutorial_on_the_BM25F_Model Acessado 16/02/2023.\n",
        "(2) Okapi BM25 - Wikipedia. https://en.wikipedia.org/wiki/Okapi_BM25 Acessado 16/02/2023.\n",
        "(3) Okapi BM25 - formulasearchengine. https://www.formulasearchengine.com/wiki/Okapi_BM25 Acessado 16/02/2023.\n",
        "(4) Practical BM25 - Part 2: The BM25 Algorithm and its Variables. https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables Acessado 16/02/2023.Claro, eu posso explicar em detalhes o que é cada elemento da fórmula acima score(D,Q) usando fórmulas em LaTeX.\n",
        "\n",
        "A fórmula score(D,Q) calcula a relevância de um documento D para uma consulta Q, usando uma função de ranqueamento baseada no modelo probabilístico²³. Ela é composta pelos seguintes elementos:\n",
        "\n",
        "- $\\mathrm{IDF}(q_i)$ é o inverso da frequência do documento do termo $q_i$, que é uma medida de quão informativo ou discriminativo é o termo para diferenciar os documentos relevantes dos não relevantes. Ele é calculado como:\n",
        "\n",
        "\n",
        "$$\n",
        "\\mathrm{IDF}(q_i) = \\log \\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}\n",
        "$$\n",
        "\n",
        "\n",
        "onde N é o número total de documentos na coleção e $n(q_i)$ é o número de documentos que contêm o termo $q_i$. O IDF aumenta à medida que o termo se torna mais raro na coleção, e diminui à medida que se torna mais comum.\n",
        "\n",
        "- $f(q_i,D)$ é a frequência do termo $q_i$ no documento D, que é uma medida de quão importante ou relevante é o termo para o conteúdo do documento. Ela pode ser simplesmente a contagem do número de vezes que o termo aparece no documento, ou alguma outra função que leve em conta a posição, a proximidade ou a ponderação dos termos.\n",
        "\n",
        "- $k_1$ e b são parâmetros ajustáveis que controlam a influência da frequência do termo e do comprimento do documento na pontuação final. Eles podem variar dependendo da coleção, da consulta ou da tarefa de recuperação. Valores típicos são $k_1 \\in [1.2, 2]$ e $b = 0.75$².\n",
        "\n",
        "- $(k_1 + 1)$ e $(k_1 \\cdot (1 - b + b \\cdot |D| / \\mathrm{avgdl}))$ são fatores de normalização que evitam que documentos muito longos ou muito curtos tenham vantagem ou desvantagem na pontuação final. Eles levam em conta o comprimento do documento D ($|D|$), comparando-o com o comprimento médio dos documentos na coleção ($\\mathrm{avgdl}$). O parâmetro b controla o grau de normalização: quanto mais próximo de 1, mais sensível ao comprimento; quanto mais próximo de 0, menos sensível.\n",
        "\n",
        "A fórmula score(D,Q) soma os produtos dos elementos acima para cada termo da consulta Q, dando assim uma pontuação global para o documento D em relação à consulta Q. Quanto maior a pontuação, maior a relevância estimada.\n",
        "\n",
        "Espero que isso tenha esclarecido os elementos da fórmula BM25.\n",
        "\n",
        "Origem: conversa com o Bing, 16/02/2023(1) (PDF) A Tutorial on the BM25F Model - ResearchGate. https://www.researchgate.net/publication/308991534_A_Tutorial_on_the_BM25F_Model Acessado 16/02/2023.\n",
        "(2) Okapi BM25 - Wikipedia. https://en.wikipedia.org/wiki/Okapi_BM25 Acessado 16/02/2023.\n",
        "(3) Okapi BM25 - formulasearchengine. https://www.formulasearchengine.com/wiki/Okapi_BM25 Acessado 16/02/2023.\n",
        "(4) Practical BM25 - Part 2: The BM25 Algorithm and its Variables. https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables Acessado 16/02/2023."
      ],
      "metadata": {
        "id": "0Hzc_XQuGNwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Aprendendo sobre BM25 com o chatGPT do new Bing"
      ],
      "metadata": {
        "id": "pcklJw_kvOZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "```\n",
        "pip install python-bm25\n",
        "```\n",
        "\n",
        "Um exemplo simples de uso dessa biblioteca é o seguinte:\n",
        "\n",
        "```python\n",
        "# Importar a classe BM25Okapi\n",
        "from bm25 import BM25Okapi\n",
        "\n",
        "# Criar uma lista de documentos como listas de palavras\n",
        "docs = [\n",
        "    [\"black\", \"cat\", \"white\", \"cat\"],\n",
        "    [\"cat\", \"outer\", \"space\"],\n",
        "    [\"wag\", \"dog\"]\n",
        "]\n",
        "\n",
        "# Criar um objeto BM25Okapi com os documentos\n",
        "bm25 = BM25Okapi(docs)\n",
        "\n",
        "# Criar uma consulta como lista de palavras\n",
        "query = [\"cat\"]\n",
        "\n",
        "# Calcular as pontuações dos documentos para a consulta usando get_scores()\n",
        "scores = bm25.get_scores(query)\n",
        "\n",
        "# Imprimir as pontuações\n",
        "print(scores)\n",
        "```\n",
        "\n",
        "A saída deste código será:\n",
        "\n",
        "```\n",
        "[2.8327298855090213, 2.8327298855090213, 0]\n",
        "```\n",
        "#  Isso significa que os dois primeiros documentos têm a mesma pontuação BM25 para a consulta \"cat\" e são mais relevantes do que o terceiro documento.\n",
        "\n",
        "# Espero que isso tenha esclarecido o que é BM25 e como implementá-lo em Python."
      ],
      "metadata": {
        "id": "PbICSYSXv91_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-HhZj86UwSIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LMNCZxhnwdu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Para criar um código Python que implemente uma busca de informação usando o BM25 e avalie o sistema na coleção CISI, você pode seguir os seguintes passos:\n",
        "\n",
        "1. Instale a biblioteca gensim, que fornece uma implementação do BM25 em Python¹. Você pode usar o comando `pip install gensim` no terminal.\n",
        "2. Baixe os arquivos da coleção CISI do site http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/. Você vai precisar dos arquivos CISI.ALL, CISI.QRY e CISI.REL, que contêm os documentos, as consultas e as relevâncias, respectivamente.\n",
        "3. Crie uma função para ler os arquivos da coleção CISI e transformá-los em listas de strings. Por exemplo:\n",
        "\n",
        "```\n",
        "def read_cisi_file(filename):\n",
        "  # Abre o arquivo no modo leitura\n",
        "  with open(filename, \"r\") as f:\n",
        "    # Lê todo o conteúdo do arquivo\n",
        "    content = f.read()\n",
        "    # Divide o conteúdo em registros separados pelo caractere \".I\"\n",
        "    records = content.split(\".I\")\n",
        "    # Remove o primeiro registro vazio\n",
        "    records.pop(0)\n",
        "    # Cria uma lista vazia para armazenar os textos dos registros\n",
        "    texts = []\n",
        "    # Para cada registro na lista de registros\n",
        "    for record in records:\n",
        "      # Divide o registro em linhas\n",
        "      lines = record.split(\"\\n\")\n",
        "      # Remove a primeira linha que contém o número do registro\n",
        "      lines.pop(0)\n",
        "      # Junta as linhas restantes em um único texto\n",
        "      text = \" \".join(lines)\n",
        "      # Adiciona o texto à lista de textos\n",
        "      texts.append(text)\n",
        "  # Retorna a lista de textos\n",
        "  return texts\n",
        "\n",
        "```\n",
        "\n",
        "4. Use a função criada para ler os documentos, as consultas e as relevâncias da coleção CISI e armazená-los em variáveis. Por exemplo:\n",
        "\n",
        "```\n",
        "# Lê os documentos da coleção CISI e armazena na variável docs\n",
        "docs = read_cisi_file(\"CISI.ALL\")\n",
        "# Lê as consultas da coleção CISI e armazena na variável queries\n",
        "queries = read_cisi_file(\"CISI.QRY\")\n",
        "# Lê as relevâncias da coleção CISI e armazena na variável relevances\n",
        "relevances = read_cisi_file(\"CISI.REL\")\n",
        "\n",
        "```\n",
        "\n",
        "5. Crie uma função para pré-processar os textos dos documentos e das consultas, removendo pontuação, caracteres especiais, números, stopwords e aplicando stemização. Você pode usar a biblioteca nltk para isso. Por exemplo:\n",
        "\n",
        "```\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Define uma lista de stopwords em inglês\n",
        "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "# Define um stemizador baseado no algoritmo de Porter\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "def preprocess(text):\n",
        "  # Converte o texto para minúsculas\n",
        "  text = text.lower()\n",
        "  # Remove pontuação, caracteres especiais e números usando expressões regulares\n",
        "  text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "  text = re.sub(r\"\\d+\", \"\", text)\n",
        "  # Divide o texto em tokens (palavras) usando espaços em branco como separador\n",
        "  tokens = text.split()\n",
        "  # Remove os tokens que são stopwords usando list comprehension \n",
        "  tokens = [token for token in tokens if token not in stopwords]\n",
        "  # Aplica stemização aos tokens usando list comprehension \n",
        "  tokens = [stemmer.stem(token) for token in tokens]\n",
        "  # Retorna a lista de tokens pré-processados \n",
        "  return tokens\n",
        "\n",
        "```\n",
        "\n",
        "6. Use a função criada para pré-processar os documentos e as consultas da coleção CISI e armazená-los em novas variáveis. Por exemplo:\n",
        "\n",
        "```\n",
        "# Pré-processa os documentos da coleção CISI e armazena na variável docs_tokens \n",
        "docs_tokens = [preprocess(doc) for doc in docs]\n",
        "# Pré-processa as consultas da coleção CISI e armazena na variável queries_tokens \n",
        "queries_tokens = [preprocess(query) for query in queries]\n",
        "\n",
        "```\n",
        "\n",
        "7. Importe a classe BM25 do módulo gensim.summarization.bm25¹ e use-a para criar um objeto BM25 com base nos documentos pré-processados. Por exemplo:\n",
        "\n",
        "```\n",
        "from gensim.summarization.b\n",
        "\n",
        "Origem: conversa com o Bing, 16/02/2023(1) How to use gensim BM25 ranking in python - Stack Overflow. https://stackoverflow.com/questions/40966014/how-to-use-gensim-bm25-ranking-in-python Acessado 16/02/2023.\n",
        "(2) GitHub - nhirakawa/BM25: A Python implementation of the BM25 ranking .... https://github.com/nhirakawa/BM25 Acessado 16/02/2023.\n",
        "(3) BM25 | Build your Own NLP Based Search Engine Using BM25 - Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25/ Acessado 16/02/2023."
      ],
      "metadata": {
        "id": "llFy7c98R0on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Here is a Python code that implements a information search retrieval using BM25 and evaluates the system on CISI collection. I have added comments to explain the details.\n",
        "\n",
        "```python\n",
        "# Importing libraries\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from gensim import corpora, models, similarities\n",
        "\n",
        "# Defining a function to read and tokenize documents from a file\n",
        "def read_docs(file):\n",
        "    docs = []\n",
        "    with open(file, \"r\") as f:\n",
        "        for line in f:\n",
        "            # Each document starts with .I followed by an id number\n",
        "            if line.startswith(\".I\"):\n",
        "                doc = \"\"\n",
        "                doc_id = line.strip()\n",
        "            # Each document ends with .X followed by references\n",
        "            elif line.startswith(\".X\"):\n",
        "                docs.append((doc_id, doc.split()))\n",
        "            else:\n",
        "                # Removing punctuation and numbers from the text\n",
        "                line = re.sub(\"[^a-zA-Z\\s]\", \"\", line)\n",
        "                doc += line.lower()\n",
        "    return docs\n",
        "\n",
        "# Defining a function to read and tokenize queries from a file\n",
        "def read_queries(file):\n",
        "    queries = []\n",
        "    with open(file, \"r\") as f:\n",
        "        for line in f:\n",
        "            # Each query starts with .I followed by an id number\n",
        "            if line.startswith(\".I\"):\n",
        "                query = \"\"\n",
        "                query_id = line.strip()\n",
        "            # Each query ends with .W followed by text\n",
        "            elif line.startswith(\".W\"):\n",
        "                query += f.readline().lower()\n",
        "                queries.append((query_id, query.split()))\n",
        "    return queries\n",
        "\n",
        "# Defining a function to read relevance judgments from a file\n",
        "def read_relevance(file):\n",
        "    relevance = {}\n",
        "    with open(file, \"r\") as f:\n",
        "        for line in f:\n",
        "            # Each line contains a query id, a document id and a relevance score (1 or 0)\n",
        "            qid, did, rel = line.split()\n",
        "            # Storing the relevance judgments in a dictionary of sets\n",
        "            if qid not in relevance:\n",
        "                relevance[qid] = set()\n",
        "            if rel == \"1\":\n",
        "                relevance[qid].add(did)\n",
        "    return relevance\n",
        "\n",
        "# Reading and tokenizing the documents from cisi.all file\n",
        "docs = read_docs(\"cisi.all\")\n",
        "\n",
        "# Creating a dictionary of terms from the documents\n",
        "dictionary = corpora.Dictionary(doc[1] for doc in docs)\n",
        "\n",
        "# Creating a corpus of bag-of-words vectors from the documents\n",
        "corpus = [dictionary.doc2bow(doc[1]) for doc in docs]\n",
        "\n",
        "# Initializing a BM25 model from gensim library using the corpus and dictionary \n",
        "bm25_model = models.Bm25Model(corpus=corpus, id2word=dictionary)\n",
        "\n",
        "# Reading and tokenizing the queries from cisi.que file \n",
        "queries = read_queries(\"cisi.que\")\n",
        "\n",
        "# Reading the relevance judgments from cisi.rel file \n",
        "relevance = read_relevance(\"cisi.rel\")\n",
        "\n",
        "# Defining some evaluation metrics \n",
        "precision_at_10 = 0 # Precision at rank 10 \n",
        "average_precision = 0 # Mean average precision \n",
        "recall_at_50 = 0 # Recall at rank 50 \n",
        "num_queries = len(queries) # Number of queries \n",
        "\n",
        "# Looping through each query \n",
        "for query in queries:\n",
        "    # Getting the query id and terms \n",
        "    qid, qterms = query \n",
        "    # Converting the query terms to bag-of-words vector \n",
        "    qvec = dictionary.doc2bow(qterms) \n",
        "    # Getting the BM25 scores for each document in the corpus using the query vector \n",
        "    scores = bm25_model.get_scores(qvec) \n",
        "    # Sorting the scores in descending order and getting the top 50 documents ids  \n",
        "    ranked_docs_ids= sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:50]  \n",
        "    ranked_docs_ids= [docs[i][0] for i in ranked_docs_ids]  \n",
        "   \n",
        "   ### Evaluating precision at rank 10 ###\n",
        "   relevant_at_10= set(ranked_docs_ids[:10]) & relevance[qid]  \n",
        "   precision_at_10+= len(relevant_at_10) / 10  \n",
        "\n",
        "   ### Evaluating mean average precision ###\n",
        "   num_relevant= 0  \n",
        "   sum_precision= 0  \n",
        "   for i, didin enumerate(ranked_docs_ids):  \n",
        "       if didinrelevance[qid]:  \n",
        "           num_relevant+= 1  \n",
        "           sum_precision+= num_relevant / (i + 1)  \n",
        "   average_precision+= sum_precision / len(relevance[qid])  \n",
        "\n",
        "   ### Evaluating recall at rank 50 ###\n",
        "   relevant_at_50= set(ranked_docs_ids) & relevance[qid]\n",
        "\n",
        "Origem: conversa com o Bing, 16/02/2023(1) How to use gensim BM25 ranking in python - Stack Overflow. https://stackoverflow.com/questions/40966014/how-to-use-gensim-bm25-ranking-in-python Acessado 16/02/2023.\n",
        "(2) BM25 | Build your Own NLP Based Search Engine Using BM25 - Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25/ Acessado 16/02/2023.\n",
        "(3) svm - Implementation of Okapi BM25 in python - Stack Overflow. https://stackoverflow.com/questions/61877065/implementation-of-okapi-bm25-in-python Acessado 16/02/2023.\n",
        "(4) Glasgow IDOM - CISI collection. http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/ Acessado 16/02/2023.\n",
        "(5) Average precisions for CISI collection: 1000 documents are added in 100 .... https://www.researchgate.net/figure/Average-precisions-for-CISI-collection-1000-documents-are-added-in-100-groups-of-10_fig2_228961662 Acessado 16/02/2023.\n",
        "(6) Comparison of normalized error for the CISI collection: 1000 documents .... https://www.researchgate.net/figure/Comparison-of-normalized-error-for-the-CISI-collection-1000-documents-are-added-in-100_fig7_228961662 Acessado 16/02/2023.\n",
        "(7) How to use gensim BM25 ranking in python - Stack Overflow. https://stackoverflow.com/questions/40966014/how-to-use-gensim-bm25-ranking-in-python Acessado 16/02/2023.\n",
        "(8) rank-bm25 · PyPI. https://pypi.org/project/rank-bm25/ Acessado 16/02/2023.\n",
        "(9) BM25 | Build your Own NLP Based Search Engine Using BM25 - Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25/ Acessado 16/02/2023."
      ],
      "metadata": {
        "id": "ftjH6imvT5re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BM25 is a ranking function that estimates the relevance of documents to a given search query based on the frequency and importance of the query terms in the documents². It is a bag-of-words retrieval function, which means it does not consider the order or proximity of the query terms within the document². It is based on the probabilistic retrieval framework developed by Stephen E. Robertson, Karen Spärck Jones, and others²⁵.\n",
        "\n",
        "Origem: conversa com o Bing, 16/02/2023(1) Okapi BM25 - Wikipedia. https://en.wikipedia.org/wiki/Okapi_BM25 Acessado 16/02/2023.\n",
        "(2) How does BM25 work? - Quora. https://www.quora.com/How-does-BM25-work Acessado 16/02/2023.\n",
        "(3) BM25 | Build your Own NLP Based Search Engine Using BM25 - Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25/#:~:text=BM25%20is%20a%20simple%20Python%20package%20and%20can,occurrences%20of%20the%20search%20term%20in%20our%20tweet Acessado 16/02/2023.\n",
        "(4) BM25 | Build your Own NLP Based Search Engine Using BM25 - Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25/ Acessado 16/02/2023.\n",
        "(5) Practical BM25 - Part 2: The BM25 Algorithm and its Variables. https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables Acessado 16/02/2023."
      ],
      "metadata": {
        "id": "7GUt5H7sUKKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A information search using BM25 works as follows:\n",
        "\n",
        "- First, you need to have a collection of documents that you want to search through. These documents can be anything from web pages, tweets, news articles, etc. You also need to have a query that represents what you are looking for.\n",
        "- Second, you need to preprocess both the documents and the query by tokenizing them into words, removing stopwords and punctuation, stemming or lemmatizing them, etc. This will help reduce the noise and variability in the text data.\n",
        "- Third, you need to create a dictionary of terms from the documents and assign each term an id. You also need to create a corpus of bag-of-words vectors from the documents, where each vector represents how many times each term appears in each document.\n",
        "- Fourth, you need to initialize a BM25 model using the corpus and dictionary. The BM25 model will calculate some statistics for each term and document, such as term frequency (TF), inverse document frequency (IDF), document length (DL), average document length (AVDL), etc. These statistics will be used to compute the BM25 score for each document given a query.\n",
        "- Fifth, you need to convert your query into a bag-of-words vector using the same dictionary. Then, you need to use the BM25 model to get the scores for each document in the corpus using the query vector. The scores are based on a formula that combines TF, IDF, DL, AVDL and some parameters that control how much weight is given to each factor. The higher the score, the more relevant the document is to the query.\n",
        "- Sixth, you need to sort the scores in descending order and get a ranked list of documents that match your query. You can also limit the number of documents returned or apply some filters or thresholds based on your needs.\n",
        "\n",
        "Here is a simple python code that illustrates these steps using gensim library:\n",
        "\n",
        "```python\n",
        "# Importing gensim library\n",
        "from gensim import corpora, models\n",
        "\n",
        "# Defining some sample documents\n",
        "docs = [\"The sky is blue.\", \"The sun is bright.\", \"The grass is green.\", \"The snow is white.\"]\n",
        "\n",
        "# Defining a sample query\n",
        "query = \"What color is snow?\"\n",
        "\n",
        "# Preprocessing both documents and query by tokenizing them into words\n",
        "docs = [doc.lower().split() for doc in docs]\n",
        "query = query.lower().split()\n",
        "\n",
        "# Creating a dictionary of terms from documents\n",
        "dictionary = corpora.Dictionary(docs)\n",
        "\n",
        "# Creating a corpus of bag-of-words vectors from documents\n",
        "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
        "\n",
        "# Initializing a BM25 model using corpus and dictionary\n",
        "bm25_model = models.Bm25Model(corpus=corpus,id2word=dictionary)\n",
        "\n",
        "# Converting query into bag-of-words vector using dictionary\n",
        "query_vec = dictionary.doc2bow(query)\n",
        "\n",
        "# Getting BM25 scores for each document in corpus using query vector\n",
        "scores = bm25_model.get_scores(query_vec)\n",
        "\n",
        "# Sorting scores in descending order and getting ranked list of documents ids \n",
        "ranked_docs_ids = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
        "\n",
        "# Printing ranked list of documents with their scores \n",
        "for i in ranked_docs_ids:\n",
        "    print(f\"Document {i}: {docs[i]} - Score: {scores[i]}\")\n",
        "```\n",
        "\n",
        "Origem: conversa com o Bing, 17/02/2023(1) nlp - Using BM25 to rank words - Data Science Stack Exchange. https://datascience.stackexchange.com/questions/75839/using-bm25-to-rank-words#:~:text=BM25%20is%20usually%20used%20in%20information%20retrieval.%20In,provided%20from%20the%20most%20relevant%20to%20the%20least. Acessado 17/02/2023.\n",
        "(2) BM25 | Build your Own NLP Based Search Engine Using BM25 - Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25/ Acessado 17/02/2023.\n",
        "(3) Okapi BM25 - Wikipedia. https://en.wikipedia.org/wiki/Okapi_BM25 Acessado 17/02/2023.\n",
        "(4) BM25 using Python Gensim Package | Search Engine. https://iamgeekydude.com/2022/12/25/bm25-using-python-gensim-package-search-engine-nlp/ Acessado 17/02/2023.\n",
        "(5) Practical BM25 - Part 2: The BM25 Algorithm and its Variables. https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables Acessado 17/02/2023."
      ],
      "metadata": {
        "id": "gvr2sAp6VJG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To modify the code to work with any collection, you need to do the following steps:\n",
        "\n",
        "- First, you need to load your collection of documents from a file or a database. You can use any format that suits your needs, such as JSON, CSV, XML, etc. You can also use any library that helps you read and parse your data, such as pandas, json, xmltodict, etc.\n",
        "- Second, you need to preprocess your documents as before by tokenizing them into words, removing stopwords and punctuation, stemming or lemmatizing them, etc. You can use any library that helps you with these tasks, such as nltk, spacy, gensim.utils.simple_preprocess(), etc.\n",
        "- Third, you need to create a dictionary of terms from your documents using gensim.corpora.Dictionary(). This will assign each term an id and store some statistics about the term frequency and document frequency.\n",
        "- Fourth, you need to create a corpus of bag-of-words vectors from your documents using dictionary.doc2bow(). This will create a list of tuples for each document where each tuple contains a term id and its count in the document.\n",
        "- Fifth, you need to initialize a BM25 model using gensim.summarization.bm25.BM25Model(). This will take the corpus and dictionary as input and calculate some statistics for each term and document that will be used for scoring.\n",
        "- Sixth, you need to convert your query into a bag-of-words vector using dictionary.doc2bow() as before. Then, you need to use bm25_model.get_scores() to get the scores for each document in the corpus using the query vector. The scores are based on a formula that combines TF,IDF,Dl,AIDL and some parameters that control how much weight is given to each factor. The higher the score,the more relevantthe document is tothe query.\n",
        "-Seventh,youneedtosortthescoresindescendingorderandgetarankedlistofdocumentsidsusing sorted().Youcanalsolimitthenumberofdocumentsreturnedorapplysomefiltersorthresholdsbasedonyourneeds.\n",
        "\n",
        "Here is an example of modifying the code to work with any collection:\n",
        "\n",
        "```python\n",
        "# Importing libraries\n",
        "import json\n",
        "from gensim import corpora\n",
        "from gensim.summarization import bm25\n",
        "\n",
        "# Loading collection of documents from JSON file\n",
        "with open(\"docs.json\", \"r\") as f:\n",
        "    docs = json.load(f)\n",
        "\n",
        "# Preprocessing documents by tokenizing them into words\n",
        "docs = [doc.lower().split() for doc in docs]\n",
        "\n",
        "# Creating a dictionary of terms from documents\n",
        "dictionary = corpora.Dictionary(docs)\n",
        "\n",
        "# Creating a corpus of bag-of-words vectors from documents\n",
        "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
        "\n",
        "# Initializing a BM25 model using corpus and dictionary\n",
        "bm25_model = bm25.BM25Model(corpus=corpus,id2word=dictionary)\n",
        "\n",
        "# Defining a sample query\n",
        "query = \"What color is snow?\"\n",
        "\n",
        "# Converting query into bag-of-words vector using dictionary\n",
        "query_vec = dictionary.doc2bow(query.split())\n",
        "\n",
        "# Getting BM25 scores for each document in corpus using query vector\n",
        "scores = bm25_model.get_scores(query_vec)\n",
        "\n",
        "# Sorting scores in descending order and getting ranked list of documents ids \n",
        "ranked_docs_ids = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
        "\n",
        "# Printing ranked list of documents with their scores \n",
        "for i in ranked_docs_ids:\n",
        "    print(f\"Document {i}: {docs[i]} - Score: {scores[i]}\")\n",
        "```\n",
        "\n",
        "Origem: conversa com o Bing, 17/02/2023(1) Configure relevance scoring - Azure Cognitive Search | Microsoft Learn. https://learn.microsoft.com/en-us/azure/search/index-ranking-similarity Acessado 17/02/2023.\n",
        "(2) How to use gensim BM25 ranking in python - Stack Overflow. https://stackoverflow.com/questions/40966014/how-to-use-gensim-bm25-ranking-in-python Acessado 17/02/2023.\n",
        "(3) Practical BM25 - Part 2: The BM25 Algorithm and its Variables. https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables Acessado 17/02/2023."
      ],
      "metadata": {
        "id": "XlWzNSUsVmvK"
      }
    }
  ]
}