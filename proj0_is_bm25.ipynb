{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/cP4ohhv5JjmuQ+BVreHY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rprimi/IR_bm25/blob/main/proj0_is_bm25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Building a Simple Information Retrieval System using BM25\n",
        "\n",
        "## Description\n",
        "\n",
        "Exerc√≠cio para sele√ß√£o aluno especial - Curso \"Deep Learning aplicado a sistemas de buscas\", FEEC-Unicamp.\n",
        "\n",
        "**Objectives:** \n",
        "- Create basics of Information Retrieval (IR) andusing the BM25 ranking algorithm in Python.\n",
        "\n",
        "**Deliverables:**\n",
        "- A github repository\n",
        "- Google Colab notebooks and associated functions and libraries if any, with the code for the IR system\n",
        "- A brief report describing the implementation details, results, how to test the IR system and how chatGPT helped you with the project.\n",
        "- Evaluation of the model in the CISI collection\n",
        "http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/\n",
        "\n",
        "**Grading:**  \n",
        "The project will be evaluated based on the following criteria:\n",
        "- Completion of the project deliverables (partial is OK to submit)\n",
        "- Implementation accuracy\n",
        "- Quality of the report\n",
        "- Code readability and commenting\n",
        "- How chatGPT was used\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZfrttcD9tn85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning about BM25\n",
        "Prompts for Bing ChatGPT4: "
      ],
      "metadata": {
        "id": "P2rv0vlkPKOu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "proUyqpuLsA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Claro, eu posso explicar em detalhes o que √© cada elemento da f√≥rmula acima score(D,Q) usando f√≥rmulas em LaTeX.\n",
        "\n",
        "A f√≥rmula score(D,Q) calcula a relev√¢ncia de um documento D para uma consulta Q, usando uma fun√ß√£o de ranqueamento baseada no modelo probabil√≠stico¬≤¬≥. Ela √© composta pelos seguintes elementos:\n",
        "\n",
        "- $\\mathrm{IDF}(q_i)$ √© o inverso da frequ√™ncia do documento do termo $q_i$, que √© uma medida de qu√£o informativo ou discriminativo √© o termo para diferenciar os documentos relevantes dos n√£o relevantes. Ele √© calculado como:\n",
        "\n",
        "\n",
        "$$\n",
        "\\mathrm{IDF}(q_i) = \\log \\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}\n",
        "$$\n",
        "\n",
        "\n",
        "onde N √© o n√∫mero total de documentos na cole√ß√£o e $n(q_i)$ √© o n√∫mero de documentos que cont√™m o termo $q_i$. O IDF aumenta √† medida que o termo se torna mais raro na cole√ß√£o, e diminui √† medida que se torna mais comum.\n",
        "\n",
        "- $f(q_i,D)$ √© a frequ√™ncia do termo $q_i$ no documento D, que √© uma medida de qu√£o importante ou relevante √© o termo para o conte√∫do do documento. Ela pode ser simplesmente a contagem do n√∫mero de vezes que o termo aparece no documento, ou alguma outra fun√ß√£o que leve em conta a posi√ß√£o, a proximidade ou a pondera√ß√£o dos termos.\n",
        "\n",
        "- $k_1$ e b s√£o par√¢metros ajust√°veis que controlam a influ√™ncia da frequ√™ncia do termo e do comprimento do documento na pontua√ß√£o final. Eles podem variar dependendo da cole√ß√£o, da consulta ou da tarefa de recupera√ß√£o. Valores t√≠picos s√£o $k_1 \\in [1.2, 2]$ e $b = 0.75$¬≤.\n",
        "\n",
        "- $(k_1 + 1)$ e $(k_1 \\cdot (1 - b + b \\cdot |D| / \\mathrm{avgdl}))$ s√£o fatores de normaliza√ß√£o que evitam que documentos muito longos ou muito curtos tenham vantagem ou desvantagem na pontua√ß√£o final. Eles levam em conta o comprimento do documento D ($|D|$), comparando-o com o comprimento m√©dio dos documentos na cole√ß√£o ($\\mathrm{avgdl}$). O par√¢metro b controla o grau de normaliza√ß√£o: quanto mais pr√≥ximo de 1, mais sens√≠vel ao comprimento; quanto mais pr√≥ximo de 0, menos sens√≠vel.\n",
        "\n",
        "A f√≥rmula score(D,Q) soma os produtos dos elementos acima para cada termo da consulta Q, dando assim uma pontua√ß√£o global para o documento D em rela√ß√£o √† consulta Q. Quanto maior a pontua√ß√£o, maior a relev√¢ncia estimada.\n",
        "\n",
        "Espero que isso tenha esclarecido os elementos da f√≥rmula BM25.\n",
        "\n",
        "Origem: conversa com o Bing, 16/02/2023(1) (PDF) A Tutorial on the BM25F Model - ResearchGate. https://www.researchgate.net/publication/308991534_A_Tutorial_on_the_BM25F_Model Acessado 16/02/2023.\n",
        "(2) Okapi BM25 - Wikipedia. https://en.wikipedia.org/wiki/Okapi_BM25 Acessado 16/02/2023.\n",
        "(3) Okapi BM25 - formulasearchengine. https://www.formulasearchengine.com/wiki/Okapi_BM25 Acessado 16/02/2023.\n",
        "(4) Practical BM25 - Part 2: The BM25 Algorithm and its Variables. https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables Acessado 16/02/2023."
      ],
      "metadata": {
        "id": "0Hzc_XQuGNwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Aprendendo sobre BM25 com o chatGPT do new Bing"
      ],
      "metadata": {
        "id": "pcklJw_kvOZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ol√°, este √© o Bing. Eu posso te ajudar. üòä\n",
        "\n",
        "BM25 √© uma fun√ß√£o de classifica√ß√£o usada por mecanismos de busca para estimar a relev√¢ncia de documentos para uma determinada consulta de pesquisa¬π¬≤. √â baseada no modelo probabil√≠stico de recupera√ß√£o de informa√ß√£o desenvolvido nas d√©cadas de 1970 e 1980 por Stephen E. Robertson, Karen Sp√§rck Jones e outros¬≤.\n",
        "\n",
        "BM25 √© uma fun√ß√£o que pontua um conjunto de documentos com base nos termos da consulta que aparecem em cada documento, independentemente da proximidade entre os termos dentro do documento¬≤. √â uma fam√≠lia de fun√ß√µes de pontua√ß√£o com componentes e par√¢metros ligeiramente diferentes. Uma das inst√¢ncias mais proeminentes da fun√ß√£o √© a seguinte¬≤:\n",
        "\n",
        "$$\n",
        "\\mathrm{score}(D,Q) = \\sum_{i=1}^{n} \\mathrm{IDF}(q_i) \\cdot \\frac{f(q_i,D) \\cdot (k_1 + 1)}{f(q_i,D) + k_1 \\cdot (1 - b + b \\cdot |D| / \\mathrm{avgdl})}\n",
        "$$\n",
        "\n",
        "\n",
        "onde:\n",
        "\n",
        "- `D` √© um documento\n",
        "- `Q` √© uma consulta\n",
        "- `f(q_i,D)` √© a frequ√™ncia do termo `q_i` no documento `D`\n",
        "- `|D|` √© o comprimento do documento `D` em palavras\n",
        "- `avgdl` √© o comprimento m√©dio dos documentos na cole√ß√£o\n",
        "- `k_1` e `b` s√£o par√¢metros livres, geralmente escolhidos como `k_1 = 2,0` e `b = 0,75`\n",
        "- `IDF(q_i)` √© o peso do termo inverso da frequ√™ncia do documento para o termo `q_i`, calculado como:\n",
        "\n",
        "```\n",
        "IDF(q_i) = log((N - n(q_i) + 0.5) / (n(q_i) + 0.5))\n",
        "```\n",
        "\n",
        "onde:\n",
        "\n",
        "- `N` √© o n√∫mero total de documentos na cole√ß√£o\n",
        "- `n(q_i)` √© o n√∫mero de documentos que cont√™m o termo `q_i`\n",
        "\n",
        "A ideia por tr√°s do BM25 √© que quanto mais vezes um termo da consulta aparece em um documento, maior ser√° a relev√¢ncia desse documento para a consulta. No entanto, essa relev√¢ncia tamb√©m depende da frequ√™ncia do termo na cole√ß√£o inteira (quanto mais raro for o termo, maior ser√° seu peso), do comprimento do documento (documentos mais curtos tendem a ter pontua√ß√µes mais altas) e dos par√¢metros livres que podem ser ajustados para diferentes cen√°rios.\n",
        "\n",
        "Existem v√°rias implementa√ß√µes em Python da fun√ß√£o BM25 dispon√≠veis online‚Å∂‚Å∑‚Åπ [^10^] ¬π¬π. Uma delas √© a biblioteca python-bm25‚Å∂, que pode ser instalada usando pip:\n",
        "\n",
        "\n",
        "Origem: conversa com o Bing, 16/02/2023(1) Okapi BM25 - Wikipedia. https://en.wikipedia.org/wiki/Okapi_BM25 Acessado 16/02/2023.\n",
        "(2) Okapi BM25 - Wikipedia. https://en.wikipedia.org/wiki/Okapi_BM25 Acessado 16/02/2023.\n",
        "(3) python-bm25 | Python implementation of BM25 function | Search Engine .... https://kandi.openweaver.com/python/fanta-mnix/python-bm25#:~:text=python-bm25%20is%20a%20Python%20library%20typically%20used%20in,Python%20implementation%20of%20BM25%20function%20for%20document%20retrieval Acessado 16/02/2023.\n",
        "(4) BM25 using Python Gensim Package | Search Engine. https://iamgeekydude.com/2022/12/25/bm25-using-python-gensim-package-search-engine-nlp/ Acessado 16/02/2023.\n",
        "(5) GitHub - nhirakawa/BM25: A Python implementation of the BM25 ranking .... https://github.com/nhirakawa/BM25 Acessado 16/02/2023.\n",
        "(6) GitHub - fanta-mnix/python-bm25: Python implementation of BM25 function .... https://github.com/fanta-mnix/python-bm25 Acessado 16/02/2023.\n",
        "(7) GitHub - xianchen2/Text_Retrieval_BM25: Python implementation of the .... https://github.com/xianchen2/Text_Retrieval_BM25 Acessado 16/02/2023.\n",
        "(8) Machine Learning Approach for Improved BM25 Retrieval. https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/LearningBM25MSRTechReport.pdf Acessado 16/02/2023.\n",
        "(9) Information retrieval evaluation of precision, recall, f-score, AP, MAP .... https://stackoverflow.com/questions/40457331/information-retrieval-evaluation-of-precision-recall-f-score-ap-map-in-pytho Acessado 16/02/2023.\n",
        "(10) Information Retrieval with document Re-ranking with BERT and BM25. https://medium.com/@papai143/information-retrieval-with-document-re-ranking-with-bert-and-bm25-7c29d738df73 Acessado 16/02/2023.\n",
        "(11) python-bm25 | Python implementation of BM25 function | Search Engine .... https://kandi.openweaver.com/python/fanta-mnix/python-bm25 Acessado 16/02/2023."
      ],
      "metadata": {
        "id": "Xym-jV_QvbwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "```\n",
        "pip install python-bm25\n",
        "```\n",
        "\n",
        "Um exemplo simples de uso dessa biblioteca √© o seguinte:\n",
        "\n",
        "```python\n",
        "# Importar a classe BM25Okapi\n",
        "from bm25 import BM25Okapi\n",
        "\n",
        "# Criar uma lista de documentos como listas de palavras\n",
        "docs = [\n",
        "    [\"black\", \"cat\", \"white\", \"cat\"],\n",
        "    [\"cat\", \"outer\", \"space\"],\n",
        "    [\"wag\", \"dog\"]\n",
        "]\n",
        "\n",
        "# Criar um objeto BM25Okapi com os documentos\n",
        "bm25 = BM25Okapi(docs)\n",
        "\n",
        "# Criar uma consulta como lista de palavras\n",
        "query = [\"cat\"]\n",
        "\n",
        "# Calcular as pontua√ß√µes dos documentos para a consulta usando get_scores()\n",
        "scores = bm25.get_scores(query)\n",
        "\n",
        "# Imprimir as pontua√ß√µes\n",
        "print(scores)\n",
        "```\n",
        "\n",
        "A sa√≠da deste c√≥digo ser√°:\n",
        "\n",
        "```\n",
        "[2.8327298855090213, 2.8327298855090213, 0]\n",
        "```\n",
        "#  Isso significa que os dois primeiros documentos t√™m a mesma pontua√ß√£o BM25 para a consulta \"cat\" e s√£o mais relevantes do que o terceiro documento.\n",
        "\n",
        "# Espero que isso tenha esclarecido o que √© BM25 e como implement√°-lo em Python."
      ],
      "metadata": {
        "id": "PbICSYSXv91_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-HhZj86UwSIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Claro, eu posso explicar cada elemento da f√≥rmula BM25.\n",
        "\n",
        "A f√≥rmula BM25 √© a seguinte¬≤:\n",
        "\n",
        "$$\n",
        "score(D,Q) = sum_{i=1}^{n} IDF(q_i) * (f(q_i,D) * (k_1 + 1)) / (f(q_i,D) + k_1 * (1 - b + b * |D| / avgdl))\n",
        "$$\n",
        "\n",
        "\n",
        "Os elementos da f√≥rmula s√£o:\n",
        "\n",
        "- `score(D,Q)` √© a pontua√ß√£o BM25 do documento `D` para a consulta `Q`\n",
        "- `sum_{i=1}^{n}` √© a soma sobre todos os termos da consulta `q_i` que aparecem no documento `D`\n",
        "- `IDF(q_i)` √© o peso do termo inverso da frequ√™ncia do documento para o termo `q_i`, que mede o qu√£o informativo √© o termo para a consulta. √â calculado como:\n",
        "\n",
        "$$IDF(q_i) = log((N - n(q_i) + 0.5) / (n(q_i) + 0.5))$$\n",
        "\n",
        "\n",
        "onde:\n",
        "\n",
        "- $N$ √© o n√∫mero total de documentos na cole√ß√£o\n",
        "- `n(q_i)` √© o n√∫mero de documentos que cont√™m o termo `q_i`\n",
        "\n",
        "Quanto menor for o valor de `n(q_i)`, maior ser√° o valor de `IDF(q_i)` e maior ser√° a contribui√ß√£o do termo para a pontua√ß√£o final.\n",
        "\n",
        "- `(f(q_i,D) * (k_1 + 1)) / (f(q_i,D) + k_1 * (1 - b + b * |D| / avgdl))` √© um fator de normaliza√ß√£o que leva em conta a frequ√™ncia do termo no documento, o comprimento do documento e os par√¢metros livres. √â composto por:\n",
        "\n",
        "  - `f(q_i,D)` √© a frequ√™ncia do termo `q_i` no documento `D`, ou seja, quantas vezes ele aparece no texto. Quanto maior for esse valor, maior ser√° a relev√¢ncia do documento para o termo.\n",
        "  - `(k_1 + 1)` e `(k_1 * (1 - b + b * |D| / avgdl))` s√£o dois componentes que ajustam a influ√™ncia da frequ√™ncia do termo na pontua√ß√£o final. Eles dependem dos par√¢metros livres:\n",
        "    - `k_1` controla o quanto a pontua√ß√£o aumenta com base na frequ√™ncia do termo. Valores maiores de `k_1` significam que as diferen√ßas na frequ√™ncia do termo t√™m mais impacto na pontua√ß√£o, enquanto valores menores significam que as diferen√ßas t√™m menos impacto. O valor padr√£o geralmente usado √© `k_1 = 2,0`.\n",
        "    - `b` controla o quanto a pontua√ß√£o √© afetada pelo comprimento do documento em rela√ß√£o ao comprimento m√©dio dos documentos na cole√ß√£o. Valores maiores de `b` favorecem documentos mais curtos, enquanto valores menores favorecem documentos mais longos. O valor padr√£o geralmente usado √© `b = 0,75`.\n",
        "    - |D|√© o comprimento do documento D em palavras\n",
        "    - avgdl√© o comprimento m√©dio dos documentos na cole√ß√£o\n",
        "\n",
        "Esses elementos juntos formam uma fun√ß√£o de classifica√ß√£o que tenta estimar qu√£o relevante √© um documento para uma consulta com base nos termos que eles compartilham.\n",
        "\n",
        "Espero que isso tenha esclarecido os elementos da f√≥rmula BM25.\n",
        "\n",
        "Origem: conversa com o Bing, 16/02/2023(1) Okapi BM25 - Wikipedia. https://en.wikipedia.org/wiki/Okapi_BM25 Acessado 16/02/2023.\n",
        "(2) Practical BM25 - Part 2: The BM25 Algorithm and its Variables. https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables Acessado 16/02/2023.\n",
        "(3) Document similarities with BM25 algorithm - MATLAB bm25Similarity. https://www.mathworks.com/help/textanalytics/ref/bm25similarity.html Acessado 16/02/2023."
      ],
      "metadata": {
        "id": "LMNCZxhnwdu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Para criar um c√≥digo Python que implemente uma busca de informa√ß√£o usando o BM25 e avalie o sistema na cole√ß√£o CISI, voc√™ pode seguir os seguintes passos:\n",
        "\n",
        "1. Instale a biblioteca gensim, que fornece uma implementa√ß√£o do BM25 em Python¬π. Voc√™ pode usar o comando `pip install gensim` no terminal.\n",
        "2. Baixe os arquivos da cole√ß√£o CISI do site http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/. Voc√™ vai precisar dos arquivos CISI.ALL, CISI.QRY e CISI.REL, que cont√™m os documentos, as consultas e as relev√¢ncias, respectivamente.\n",
        "3. Crie uma fun√ß√£o para ler os arquivos da cole√ß√£o CISI e transform√°-los em listas de strings. Por exemplo:\n",
        "\n",
        "```\n",
        "def read_cisi_file(filename):\n",
        "  # Abre o arquivo no modo leitura\n",
        "  with open(filename, \"r\") as f:\n",
        "    # L√™ todo o conte√∫do do arquivo\n",
        "    content = f.read()\n",
        "    # Divide o conte√∫do em registros separados pelo caractere \".I\"\n",
        "    records = content.split(\".I\")\n",
        "    # Remove o primeiro registro vazio\n",
        "    records.pop(0)\n",
        "    # Cria uma lista vazia para armazenar os textos dos registros\n",
        "    texts = []\n",
        "    # Para cada registro na lista de registros\n",
        "    for record in records:\n",
        "      # Divide o registro em linhas\n",
        "      lines = record.split(\"\\n\")\n",
        "      # Remove a primeira linha que cont√©m o n√∫mero do registro\n",
        "      lines.pop(0)\n",
        "      # Junta as linhas restantes em um √∫nico texto\n",
        "      text = \" \".join(lines)\n",
        "      # Adiciona o texto √† lista de textos\n",
        "      texts.append(text)\n",
        "  # Retorna a lista de textos\n",
        "  return texts\n",
        "\n",
        "```\n",
        "\n",
        "4. Use a fun√ß√£o criada para ler os documentos, as consultas e as relev√¢ncias da cole√ß√£o CISI e armazen√°-los em vari√°veis. Por exemplo:\n",
        "\n",
        "```\n",
        "# L√™ os documentos da cole√ß√£o CISI e armazena na vari√°vel docs\n",
        "docs = read_cisi_file(\"CISI.ALL\")\n",
        "# L√™ as consultas da cole√ß√£o CISI e armazena na vari√°vel queries\n",
        "queries = read_cisi_file(\"CISI.QRY\")\n",
        "# L√™ as relev√¢ncias da cole√ß√£o CISI e armazena na vari√°vel relevances\n",
        "relevances = read_cisi_file(\"CISI.REL\")\n",
        "\n",
        "```\n",
        "\n",
        "5. Crie uma fun√ß√£o para pr√©-processar os textos dos documentos e das consultas, removendo pontua√ß√£o, caracteres especiais, n√∫meros, stopwords e aplicando stemiza√ß√£o. Voc√™ pode usar a biblioteca nltk para isso. Por exemplo:\n",
        "\n",
        "```\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Define uma lista de stopwords em ingl√™s\n",
        "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "# Define um stemizador baseado no algoritmo de Porter\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "def preprocess(text):\n",
        "  # Converte o texto para min√∫sculas\n",
        "  text = text.lower()\n",
        "  # Remove pontua√ß√£o, caracteres especiais e n√∫meros usando express√µes regulares\n",
        "  text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "  text = re.sub(r\"\\d+\", \"\", text)\n",
        "  # Divide o texto em tokens (palavras) usando espa√ßos em branco como separador\n",
        "  tokens = text.split()\n",
        "  # Remove os tokens que s√£o stopwords usando list comprehension \n",
        "  tokens = [token for token in tokens if token not in stopwords]\n",
        "  # Aplica stemiza√ß√£o aos tokens usando list comprehension \n",
        "  tokens = [stemmer.stem(token) for token in tokens]\n",
        "  # Retorna a lista de tokens pr√©-processados \n",
        "  return tokens\n",
        "\n",
        "```\n",
        "\n",
        "6. Use a fun√ß√£o criada para pr√©-processar os documentos e as consultas da cole√ß√£o CISI e armazen√°-los em novas vari√°veis. Por exemplo:\n",
        "\n",
        "```\n",
        "# Pr√©-processa os documentos da cole√ß√£o CISI e armazena na vari√°vel docs_tokens \n",
        "docs_tokens = [preprocess(doc) for doc in docs]\n",
        "# Pr√©-processa as consultas da cole√ß√£o CISI e armazena na vari√°vel queries_tokens \n",
        "queries_tokens = [preprocess(query) for query in queries]\n",
        "\n",
        "```\n",
        "\n",
        "7. Importe a classe BM25 do m√≥dulo gensim.summarization.bm25¬π e use-a para criar um objeto BM25 com base nos documentos pr√©-processados. Por exemplo:\n",
        "\n",
        "```\n",
        "from gensim.summarization.b\n",
        "\n",
        "Origem: conversa com o Bing, 16/02/2023(1) How to use gensim BM25 ranking in python - Stack Overflow. https://stackoverflow.com/questions/40966014/how-to-use-gensim-bm25-ranking-in-python Acessado 16/02/2023.\n",
        "(2) GitHub - nhirakawa/BM25: A Python implementation of the BM25 ranking .... https://github.com/nhirakawa/BM25 Acessado 16/02/2023.\n",
        "(3) BM25 | Build your Own NLP Based Search Engine Using BM25 - Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25/ Acessado 16/02/2023."
      ],
      "metadata": {
        "id": "llFy7c98R0on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Here is a Python code that implements a information search retrieval using BM25 and evaluates the system on CISI collection. I have added comments to explain the details.\n",
        "\n",
        "```python\n",
        "# Importing libraries\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from gensim import corpora, models, similarities\n",
        "\n",
        "# Defining a function to read and tokenize documents from a file\n",
        "def read_docs(file):\n",
        "    docs = []\n",
        "    with open(file, \"r\") as f:\n",
        "        for line in f:\n",
        "            # Each document starts with .I followed by an id number\n",
        "            if line.startswith(\".I\"):\n",
        "                doc = \"\"\n",
        "                doc_id = line.strip()\n",
        "            # Each document ends with .X followed by references\n",
        "            elif line.startswith(\".X\"):\n",
        "                docs.append((doc_id, doc.split()))\n",
        "            else:\n",
        "                # Removing punctuation and numbers from the text\n",
        "                line = re.sub(\"[^a-zA-Z\\s]\", \"\", line)\n",
        "                doc += line.lower()\n",
        "    return docs\n",
        "\n",
        "# Defining a function to read and tokenize queries from a file\n",
        "def read_queries(file):\n",
        "    queries = []\n",
        "    with open(file, \"r\") as f:\n",
        "        for line in f:\n",
        "            # Each query starts with .I followed by an id number\n",
        "            if line.startswith(\".I\"):\n",
        "                query = \"\"\n",
        "                query_id = line.strip()\n",
        "            # Each query ends with .W followed by text\n",
        "            elif line.startswith(\".W\"):\n",
        "                query += f.readline().lower()\n",
        "                queries.append((query_id, query.split()))\n",
        "    return queries\n",
        "\n",
        "# Defining a function to read relevance judgments from a file\n",
        "def read_relevance(file):\n",
        "    relevance = {}\n",
        "    with open(file, \"r\") as f:\n",
        "        for line in f:\n",
        "            # Each line contains a query id, a document id and a relevance score (1 or 0)\n",
        "            qid, did, rel = line.split()\n",
        "            # Storing the relevance judgments in a dictionary of sets\n",
        "            if qid not in relevance:\n",
        "                relevance[qid] = set()\n",
        "            if rel == \"1\":\n",
        "                relevance[qid].add(did)\n",
        "    return relevance\n",
        "\n",
        "# Reading and tokenizing the documents from cisi.all file\n",
        "docs = read_docs(\"cisi.all\")\n",
        "\n",
        "# Creating a dictionary of terms from the documents\n",
        "dictionary = corpora.Dictionary(doc[1] for doc in docs)\n",
        "\n",
        "# Creating a corpus of bag-of-words vectors from the documents\n",
        "corpus = [dictionary.doc2bow(doc[1]) for doc in docs]\n",
        "\n",
        "# Initializing a BM25 model from gensim library using the corpus and dictionary \n",
        "bm25_model = models.Bm25Model(corpus=corpus, id2word=dictionary)\n",
        "\n",
        "# Reading and tokenizing the queries from cisi.que file \n",
        "queries = read_queries(\"cisi.que\")\n",
        "\n",
        "# Reading the relevance judgments from cisi.rel file \n",
        "relevance = read_relevance(\"cisi.rel\")\n",
        "\n",
        "# Defining some evaluation metrics \n",
        "precision_at_10 = 0 # Precision at rank 10 \n",
        "average_precision = 0 # Mean average precision \n",
        "recall_at_50 = 0 # Recall at rank 50 \n",
        "num_queries = len(queries) # Number of queries \n",
        "\n",
        "# Looping through each query \n",
        "for query in queries:\n",
        "    # Getting the query id and terms \n",
        "    qid, qterms = query \n",
        "    # Converting the query terms to bag-of-words vector \n",
        "    qvec = dictionary.doc2bow(qterms) \n",
        "    # Getting the BM25 scores for each document in the corpus using the query vector \n",
        "    scores = bm25_model.get_scores(qvec) \n",
        "    # Sorting the scores in descending order and getting the top 50 documents ids  \n",
        "    ranked_docs_ids= sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:50]  \n",
        "    ranked_docs_ids= [docs[i][0] for i in ranked_docs_ids]  \n",
        "   \n",
        "   ### Evaluating precision at rank 10 ###\n",
        "   relevant_at_10= set(ranked_docs_ids[:10]) & relevance[qid]  \n",
        "   precision_at_10+= len(relevant_at_10) / 10  \n",
        "\n",
        "   ### Evaluating mean average precision ###\n",
        "   num_relevant= 0  \n",
        "   sum_precision= 0  \n",
        "   for i, didin enumerate(ranked_docs_ids):  \n",
        "       if didinrelevance[qid]:  \n",
        "           num_relevant+= 1  \n",
        "           sum_precision+= num_relevant / (i + 1)  \n",
        "   average_precision+= sum_precision / len(relevance[qid])  \n",
        "\n",
        "   ### Evaluating recall at rank 50 ###\n",
        "   relevant_at_50= set(ranked_docs_ids) & relevance[qid]\n",
        "\n",
        "Origem: conversa com o Bing, 16/02/2023(1) How to use gensim BM25 ranking in python - Stack Overflow. https://stackoverflow.com/questions/40966014/how-to-use-gensim-bm25-ranking-in-python Acessado 16/02/2023.\n",
        "(2) BM25 | Build your Own NLP Based Search Engine Using BM25 - Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25/ Acessado 16/02/2023.\n",
        "(3) svm - Implementation of Okapi BM25 in python - Stack Overflow. https://stackoverflow.com/questions/61877065/implementation-of-okapi-bm25-in-python Acessado 16/02/2023.\n",
        "(4) Glasgow IDOM - CISI collection. http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/ Acessado 16/02/2023.\n",
        "(5) Average precisions for CISI collection: 1000 documents are added in 100 .... https://www.researchgate.net/figure/Average-precisions-for-CISI-collection-1000-documents-are-added-in-100-groups-of-10_fig2_228961662 Acessado 16/02/2023.\n",
        "(6) Comparison of normalized error for the CISI collection: 1000 documents .... https://www.researchgate.net/figure/Comparison-of-normalized-error-for-the-CISI-collection-1000-documents-are-added-in-100_fig7_228961662 Acessado 16/02/2023.\n",
        "(7) How to use gensim BM25 ranking in python - Stack Overflow. https://stackoverflow.com/questions/40966014/how-to-use-gensim-bm25-ranking-in-python Acessado 16/02/2023.\n",
        "(8) rank-bm25 ¬∑ PyPI. https://pypi.org/project/rank-bm25/ Acessado 16/02/2023.\n",
        "(9) BM25 | Build your Own NLP Based Search Engine Using BM25 - Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25/ Acessado 16/02/2023."
      ],
      "metadata": {
        "id": "ftjH6imvT5re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BM25 is a ranking function that estimates the relevance of documents to a given search query based on the frequency and importance of the query terms in the documents¬≤. It is a bag-of-words retrieval function, which means it does not consider the order or proximity of the query terms within the document¬≤. It is based on the probabilistic retrieval framework developed by Stephen E. Robertson, Karen Sp√§rck Jones, and others¬≤‚Åµ.\n",
        "\n",
        "Origem: conversa com o Bing, 16/02/2023(1) Okapi BM25 - Wikipedia. https://en.wikipedia.org/wiki/Okapi_BM25 Acessado 16/02/2023.\n",
        "(2) How does BM25 work? - Quora. https://www.quora.com/How-does-BM25-work Acessado 16/02/2023.\n",
        "(3) BM25 | Build your Own NLP Based Search Engine Using BM25 - Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25/#:~:text=BM25%20is%20a%20simple%20Python%20package%20and%20can,occurrences%20of%20the%20search%20term%20in%20our%20tweet Acessado 16/02/2023.\n",
        "(4) BM25 | Build your Own NLP Based Search Engine Using BM25 - Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25/ Acessado 16/02/2023.\n",
        "(5) Practical BM25 - Part 2: The BM25 Algorithm and its Variables. https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables Acessado 16/02/2023."
      ],
      "metadata": {
        "id": "7GUt5H7sUKKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A information search using BM25 works as follows:\n",
        "\n",
        "- First, you need to have a collection of documents that you want to search through. These documents can be anything from web pages, tweets, news articles, etc. You also need to have a query that represents what you are looking for.\n",
        "- Second, you need to preprocess both the documents and the query by tokenizing them into words, removing stopwords and punctuation, stemming or lemmatizing them, etc. This will help reduce the noise and variability in the text data.\n",
        "- Third, you need to create a dictionary of terms from the documents and assign each term an id. You also need to create a corpus of bag-of-words vectors from the documents, where each vector represents how many times each term appears in each document.\n",
        "- Fourth, you need to initialize a BM25 model using the corpus and dictionary. The BM25 model will calculate some statistics for each term and document, such as term frequency (TF), inverse document frequency (IDF), document length (DL), average document length (AVDL), etc. These statistics will be used to compute the BM25 score for each document given a query.\n",
        "- Fifth, you need to convert your query into a bag-of-words vector using the same dictionary. Then, you need to use the BM25 model to get the scores for each document in the corpus using the query vector. The scores are based on a formula that combines TF, IDF, DL, AVDL and some parameters that control how much weight is given to each factor. The higher the score, the more relevant the document is to the query.\n",
        "- Sixth, you need to sort the scores in descending order and get a ranked list of documents that match your query. You can also limit the number of documents returned or apply some filters or thresholds based on your needs.\n",
        "\n",
        "Here is a simple python code that illustrates these steps using gensim library:\n",
        "\n",
        "```python\n",
        "# Importing gensim library\n",
        "from gensim import corpora, models\n",
        "\n",
        "# Defining some sample documents\n",
        "docs = [\"The sky is blue.\", \"The sun is bright.\", \"The grass is green.\", \"The snow is white.\"]\n",
        "\n",
        "# Defining a sample query\n",
        "query = \"What color is snow?\"\n",
        "\n",
        "# Preprocessing both documents and query by tokenizing them into words\n",
        "docs = [doc.lower().split() for doc in docs]\n",
        "query = query.lower().split()\n",
        "\n",
        "# Creating a dictionary of terms from documents\n",
        "dictionary = corpora.Dictionary(docs)\n",
        "\n",
        "# Creating a corpus of bag-of-words vectors from documents\n",
        "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
        "\n",
        "# Initializing a BM25 model using corpus and dictionary\n",
        "bm25_model = models.Bm25Model(corpus=corpus,id2word=dictionary)\n",
        "\n",
        "# Converting query into bag-of-words vector using dictionary\n",
        "query_vec = dictionary.doc2bow(query)\n",
        "\n",
        "# Getting BM25 scores for each document in corpus using query vector\n",
        "scores = bm25_model.get_scores(query_vec)\n",
        "\n",
        "# Sorting scores in descending order and getting ranked list of documents ids \n",
        "ranked_docs_ids = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
        "\n",
        "# Printing ranked list of documents with their scores \n",
        "for i in ranked_docs_ids:\n",
        "    print(f\"Document {i}: {docs[i]} - Score: {scores[i]}\")\n",
        "```\n",
        "\n",
        "Origem: conversa com o Bing, 17/02/2023(1) nlp - Using BM25 to rank words - Data Science Stack Exchange. https://datascience.stackexchange.com/questions/75839/using-bm25-to-rank-words#:~:text=BM25%20is%20usually%20used%20in%20information%20retrieval.%20In,provided%20from%20the%20most%20relevant%20to%20the%20least. Acessado 17/02/2023.\n",
        "(2) BM25 | Build your Own NLP Based Search Engine Using BM25 - Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25/ Acessado 17/02/2023.\n",
        "(3) Okapi BM25 - Wikipedia. https://en.wikipedia.org/wiki/Okapi_BM25 Acessado 17/02/2023.\n",
        "(4) BM25 using Python Gensim Package | Search Engine. https://iamgeekydude.com/2022/12/25/bm25-using-python-gensim-package-search-engine-nlp/ Acessado 17/02/2023.\n",
        "(5) Practical BM25 - Part 2: The BM25 Algorithm and its Variables. https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables Acessado 17/02/2023."
      ],
      "metadata": {
        "id": "gvr2sAp6VJG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To modify the code to work with any collection, you need to do the following steps:\n",
        "\n",
        "- First, you need to load your collection of documents from a file or a database. You can use any format that suits your needs, such as JSON, CSV, XML, etc. You can also use any library that helps you read and parse your data, such as pandas, json, xmltodict, etc.\n",
        "- Second, you need to preprocess your documents as before by tokenizing them into words, removing stopwords and punctuation, stemming or lemmatizing them, etc. You can use any library that helps you with these tasks, such as nltk, spacy, gensim.utils.simple_preprocess(), etc.\n",
        "- Third, you need to create a dictionary of terms from your documents using gensim.corpora.Dictionary(). This will assign each term an id and store some statistics about the term frequency and document frequency.\n",
        "- Fourth, you need to create a corpus of bag-of-words vectors from your documents using dictionary.doc2bow(). This will create a list of tuples for each document where each tuple contains a term id and its count in the document.\n",
        "- Fifth, you need to initialize a BM25 model using gensim.summarization.bm25.BM25Model(). This will take the corpus and dictionary as input and calculate some statistics for each term and document that will be used for scoring.\n",
        "- Sixth, you need to convert your query into a bag-of-words vector using dictionary.doc2bow() as before. Then, you need to use bm25_model.get_scores() to get the scores for each document in the corpus using the query vector. The scores are based on a formula that combines TF,IDF,Dl,AIDL and some parameters that control how much weight is given to each factor. The higher the score,the more relevantthe document is tothe query.\n",
        "-Seventh,youneedtosortthescoresindescendingorderandgetarankedlistofdocumentsidsusing sorted().Youcanalsolimitthenumberofdocumentsreturnedorapplysomefiltersorthresholdsbasedonyourneeds.\n",
        "\n",
        "Here is an example of modifying the code to work with any collection:\n",
        "\n",
        "```python\n",
        "# Importing libraries\n",
        "import json\n",
        "from gensim import corpora\n",
        "from gensim.summarization import bm25\n",
        "\n",
        "# Loading collection of documents from JSON file\n",
        "with open(\"docs.json\", \"r\") as f:\n",
        "    docs = json.load(f)\n",
        "\n",
        "# Preprocessing documents by tokenizing them into words\n",
        "docs = [doc.lower().split() for doc in docs]\n",
        "\n",
        "# Creating a dictionary of terms from documents\n",
        "dictionary = corpora.Dictionary(docs)\n",
        "\n",
        "# Creating a corpus of bag-of-words vectors from documents\n",
        "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
        "\n",
        "# Initializing a BM25 model using corpus and dictionary\n",
        "bm25_model = bm25.BM25Model(corpus=corpus,id2word=dictionary)\n",
        "\n",
        "# Defining a sample query\n",
        "query = \"What color is snow?\"\n",
        "\n",
        "# Converting query into bag-of-words vector using dictionary\n",
        "query_vec = dictionary.doc2bow(query.split())\n",
        "\n",
        "# Getting BM25 scores for each document in corpus using query vector\n",
        "scores = bm25_model.get_scores(query_vec)\n",
        "\n",
        "# Sorting scores in descending order and getting ranked list of documents ids \n",
        "ranked_docs_ids = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
        "\n",
        "# Printing ranked list of documents with their scores \n",
        "for i in ranked_docs_ids:\n",
        "    print(f\"Document {i}: {docs[i]} - Score: {scores[i]}\")\n",
        "```\n",
        "\n",
        "Origem: conversa com o Bing, 17/02/2023(1) Configure relevance scoring - Azure Cognitive Search | Microsoft Learn. https://learn.microsoft.com/en-us/azure/search/index-ranking-similarity Acessado 17/02/2023.\n",
        "(2) How to use gensim BM25 ranking in python - Stack Overflow. https://stackoverflow.com/questions/40966014/how-to-use-gensim-bm25-ranking-in-python Acessado 17/02/2023.\n",
        "(3) Practical BM25 - Part 2: The BM25 Algorithm and its Variables. https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables Acessado 17/02/2023."
      ],
      "metadata": {
        "id": "XlWzNSUsVmvK"
      }
    }
  ]
}