{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3sJMnp/fLMAY6zr6GRV9+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rprimi/IR_bm25/blob/main/proj0_is_bm25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Building a Simple Information Retrieval System using BM25\n",
        "\n",
        "## Description\n",
        "\n",
        "Exercise for special student selection for the Course \"Deep Learning applied to search systems\", FEEC-Unicamp.\n",
        "\n",
        "**Objectives:** \n",
        "- Create a basic system of Information Retrieval (IR) using the BM25 ranking algorithm in Python.\n",
        "\n",
        "**Deliverables:**\n",
        "- A github repository\n",
        "- Google Colab notebooks and associated functions and libraries if any, with the code for the IR system\n",
        "- A brief report describing the implementation details, results, how to test the IR system and how chatGPT helped you with the project.\n",
        "- Evaluation of the model in the CISI collection\n",
        "http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/\n",
        "\n",
        "**Grading:**  \n",
        "The project will be evaluated based on the following criteria:\n",
        "- Completion of the project deliverables (partial is OK to submit)\n",
        "- Implementation accuracy\n",
        "- Quality of the report\n",
        "- Code readability and commenting\n",
        "- How chatGPT was used\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZfrttcD9tn85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning about BM25\n",
        "\n",
        "BM25 is a ranking function used by search engines to estimate the relevance of documents for a given search query¹². It is based on the probabilistic information retrieval model developed in the 1970s and 1980s by Stephen E. Robertson, Karen Spärck Jones and others².\n",
        "\n",
        "BM25 is a function that scores a set of documents based on the query terms that appear in each document, regardless of how close the terms are within the document². It is a family of scoring functions with slightly different components and parameters. One of the most prominent instances of the function is the following²:\n",
        "\n",
        "$$\n",
        "\\mathrm{score}(D,Q) = \\sum_{i=1}^{n} \\mathrm{IDF}(q_i) \\cdot \\frac{f(q_i,D) \\cdot (k_1 + 1)}{f(q_i,D) + k_1 \\cdot (1 - b + b \\cdot D / \\mathrm{avgdl})}\n",
        "$$\n",
        "\n",
        "\n",
        "Where:\n",
        "- $score(D,Q)$ is the BM25 score of document $D$ for query $Q$\n",
        "- $\\sum_{i=1}^{n}$ is the sum over all query terms $q_i$ that appear in document $D$\n",
        "- The $IDF(q_i)$ is given by:\n",
        "\n",
        "$$\\mathrm{IDF}(q_i) = \\log{_e} \\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}$$\n",
        "- where:\n",
        "  - $N$ is the total number of documents in the collection\n",
        "  - $n(q_i)$ is the number of documents that contain the term $q_i$\n",
        "  - The smaller the value of $n(q_i)$, the greater the value of $IDF(q_i)$ and the greater the contribution of the term to the final score. The IDF increases as the term becomes rarer in the collection, and decreases as it becomes more common. \"The IDF component of our formula measures how often a term occurs in all of the documents and “penalizes” terms that are common\"\n",
        "- $f(q_i,D)$ is the frequency of the term $q_i$ in the document $D$, that is, how many times it appears in the text. The higher this value, the more relevant the document is to the term.\n",
        "- $D$ is the length of the document D in words. $\\mathrm{avgdl}$ is the average length of the documents in the collection. The length of the documento is divided by the average doc length in the denominator: $D / \\mathrm{avgdl}$ .\"The way to think about this is that the more terms in the document — at least ones not matching the query — the lower the score for the document\"²\n",
        "- $b$ controls how much the score is affected by document length relative to the average length of documents in the collection. Larger values of $b$ favor shorter documents, while smaller values favor longer documents. The commonly used default value is $b = 0.75$. \"if b is bigger, the effects of the length of the document compared to the average length are more amplified.\"\n",
        "\n",
        "- $(k_1 + 1)$ and $(k_1 * (1 - b + b * |D| / avgdl))$ are two components that adjust for the influence of term frequency on the final score. They depend on the free parameters:\n",
        "- $k_1$ controls how much the score increases based on term frequency. Larger values of $k_1$ mean that differences in term frequency have more impact on the score, while smaller values mean that differences have less impact. The commonly used default value is $k_1 = 1.2 to 2.0$.\n",
        "These elements together form a ranking function that attempts to estimate how relevant a document is to a query based on the terms they share.\n",
        "\n",
        "### References\n",
        "(1) Okapi BM25 - Wikipedia. https://en.wikipedia.org/wiki/Okapi_BM25 Acessado 16/02/2023.  \n",
        "(2) Practical BM25 - Part 2: The BM25 Algorithm and its Variables. https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables Acessado 16/02/2023.  \n",
        "(3) Document similarities with BM25 algorithm - MATLAB bm25Similarity. https://www.mathworks.com/help/textanalytics/ref/bm25similarity.html Acessado 16/02/2023.  \n",
        "(3) python-bm25 | Python implementation of BM25 function | Search Engine .... https://kandi.openweaver.com/python/fanta-mnix/python-bm25#:~:text=python-bm25%20is%20a%20Python%20library%20typically%20used%20in,Python%20implementation%20of%20BM25%20function%20for%20document%20retrieval Acessado 16/02/2023.  \n",
        "(4) BM25 using Python Gensim Package | Search Engine. https://iamgeekydude.com/2022/12/25/bm25-using-python-gensim-package-search-engine-nlp/ Acessado 16/02/2023.  \n",
        "(5) GitHub - nhirakawa/BM25: A Python implementation of the BM25 ranking .... https://github.com/nhirakawa/BM25 Acessado 16/02/2023.  \n",
        "(6) GitHub - fanta-mnix/python-bm25: Python implementation of BM25 function .... https://github.com/fanta-mnix/python-bm25 Acessado 16/02/2023.  \n",
        "(7) GitHub - xianchen2/Text_Retrieval_BM25: Python implementation of the .... https://github.com/xianchen2/Text_Retrieval_BM25 Acessado 16/02/2023.  \n",
        "(8) Machine Learning Approach for Improved BM25 Retrieval. https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/LearningBM25MSRTechReport.pdf Acessado 16/02/2023.  \n",
        "(9) Information retrieval evaluation of precision, recall, f-score, AP, MAP .... https://stackoverflow.com/questions/40457331/information-retrieval-evaluation-of-precision-recall-f-score-ap-map-in-pytho Acessado 16/02/2023.  \n",
        "(10) Information Retrieval with document Re-ranking with BERT and BM25. https://medium.com/@papai143/information-retrieval-with-document-re-ranking-with-bert-and-bm25-7c29d738df73 Acessado 16/02/2023.  \n",
        "(11) python-bm25 | Python implementation of BM25 function | Search Engine .... https://kandi.openweaver.com/python/fanta-mnix/python-bm25 Acessado 16/02/2023.  "
      ],
      "metadata": {
        "id": "Xym-jV_QvbwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### \"Vanilla\" example"
      ],
      "metadata": {
        "id": "t4FATEmPLxbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25\n",
        "import rank_bm25\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Cria documentos como lista de palavras\n",
        "docs = [\n",
        "    [\"black\", \"cat\", \"white\", \"cat\"],\n",
        "    [\"cat\", \"outer\", \"space\", \"cat\"],\n",
        "    [\"wag\", \"dog\"]\n",
        "]\n",
        "\n",
        "# Criar um objeto BM25Okapi com os documentos\n",
        "bm25 = BM25Okapi(docs)\n",
        "\n",
        "\n",
        "# Cria uma consulta como lista de palavras\n",
        "query = [[\"cat\"], [\"cat\", \"house\"]]\n",
        "\n",
        "# Calcular as pontuações dos documentos para a consulta usando get_scores()\n",
        "scores = [bm25.get_scores(query = q) for q in query] \n",
        "\n",
        "# Imprimir as pontuações\n",
        "\n",
        "print(scores)\n",
        "\n"
      ],
      "metadata": {
        "id": "PbICSYSXv91_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb22e618-716a-40b2-f084-f7ddb43ad2b8"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.8/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from rank_bm25) (1.21.6)\n",
            "[array([0.12244142, 0.12244142, 0.        ]), array([0.12244142, 0.12244142, 0.        ])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation of BM25 algotithm using Python \"raiz\" :)\n",
        "- Database description (from ChatGPT)\n",
        "\n",
        "  - The CISI (Computer and Information Science Index) database is a classic test collection widely used in the field of information retrieval for research and evaluation purposes. The database consists of a collection of documents and a set of queries, as well as relevance judgments for each query-document pair.\n",
        "\n",
        "  - The CISI database is available in several formats, including plain text, SGML, and XML. The most commonly used format is the plain text format, which consists of the following files:\n",
        "\n",
        "  - cisi.all - This file contains the full text of all the documents in the collection. Each document is separated by a line starting with the tag .I followed by a unique document ID. The text of the document follows on subsequent lines until the next .I tag.\n",
        "\n",
        "  - cisi.docs - This file contains bibliographic information for each document in the collection. Each record begins with the tag .I followed by the document ID. The bibliographic information is contained in fields such as .T (title), .A (author), .B (source), and .W (abstract). The fields are separated by the tag and enclosed in angle brackets, like <.T>, <.A>, etc.\n",
        "\n",
        "  - cisi.qry - This file contains the queries in the collection. Each query is preceded by the tag .I followed by a unique query ID. The text of the query follows on subsequent lines until the next .I tag.\n",
        "\n",
        "  - cisi.rel - This file contains the relevance judgments for each query-document pair. Each record begins with the tag .I followed by the query ID, the tag .R and the document ID of a relevant document. Multiple relevant documents are listed on subsequent lines until the next .I tag.\n",
        "\n",
        "- In summary, the CISI database has four main files: cisi.all for the document text, cisi.docs for document metadata, cisi.qry for queries, and cisi.rel for relevance judgments. The structure of each file is defined by a set of tags, which are used to identify different parts of the data.\n"
      ],
      "metadata": {
        "id": "dMcTAb2rMKVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Cria diretório no google drive\n",
        "!mkdir -p /content/drive/MyDrive/cisi\n",
        "\n",
        "# Baixa base\n",
        "!wget -P /content/drive/MyDrive/cisi/ http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/cisi.tar.gz\n",
        "\n",
        "# Descomprime arquivos\n",
        "!tar -xvzf /content/drive/MyDrive/cisi/cisi.tar.gz -C /content/drive/MyDrive/cisi/\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTLhxOjsRyIO",
        "outputId": "e1199f89-342b-42ab-c1f3-a69de14e3cf1"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "--2023-02-22 02:02:21--  http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/cisi.tar.gz\n",
            "Resolving ir.dcs.gla.ac.uk (ir.dcs.gla.ac.uk)... 130.209.240.253\n",
            "Connecting to ir.dcs.gla.ac.uk (ir.dcs.gla.ac.uk)|130.209.240.253|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 775144 (757K) [application/gzip]\n",
            "Saving to: ‘/content/drive/MyDrive/cisi/cisi.tar.gz.5’\n",
            "\n",
            "cisi.tar.gz.5       100%[===================>] 756.98K  1.36MB/s    in 0.5s    \n",
            "\n",
            "2023-02-22 02:02:22 (1.36 MB/s) - ‘/content/drive/MyDrive/cisi/cisi.tar.gz.5’ saved [775144/775144]\n",
            "\n",
            "CISI.ALL\n",
            "CISI.BLN\n",
            "CISI.QRY\n",
            "CISI.REL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "from math import log\n",
        "!pip install gensim\n",
        "!pip install nltk\n",
        "!pip install rank_bm25\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load the document text into a dictionary: The code opens the file \"cisi.all\" \n",
        "# and reads the contents into a Python dictionary called doc_text,\n",
        "# where each document ID is associated with its corresponding text.\n",
        "# this cauded a problem: \"A.I. Mikhailov, and his colleagues\"\n",
        "# I had to change  content.split(\".I\") to content.split(\".I \")\n",
        "# learned using bing chatGPT\n",
        "\n",
        "doc_text = {}\n",
        "with open(\"/content/drive/MyDrive/cisi/CISI.ALL\", \"r\") as f:\n",
        "    content = f.read()\n",
        "    records = content.split(\".I \")\n",
        "    records.pop(0)\n",
        "    lines = f.readlines()\n",
        "    for record in records:\n",
        "      # Divide o registro em linhas\n",
        "      lines = record.split(\"\\n\")\n",
        "      # guarda id\n",
        "      id = lines[0].strip()\n",
        "      # print(lines[0])\n",
        "      # print(int(id))\n",
        "      # Remove a primeira linha que contém o número do registro\n",
        "      lines.pop(0)\n",
        "      # Junta as linhas restantes em um único texto\n",
        "      text = \" \".join(lines)\n",
        "      # Adiciona o texto à lista de textos\n",
        "      doc_text[id] = text\n",
        "\n",
        "\n",
        "# Load the queries into a list: The code opens the file \"cisi.qry\" and reads the contents\n",
        "# into a list called queries, where each query is represented as a string.\n",
        "\n",
        "queries = []\n",
        "with open(\"/content/drive/MyDrive/cisi/CISI.QRY\", \"r\") as f:\n",
        "    content = f.read()\n",
        "    records = content.split(\".I\")\n",
        "    records.pop(0)\n",
        "    lines = f.readlines()\n",
        "    for record in records:\n",
        "      # Divide o registro em linhas\n",
        "      lines = record.split(\"\\n\")\n",
        "      # guarda id\n",
        "      id = lines[0].strip()\n",
        "      # print(lines[0])\n",
        "      # print(int(id))\n",
        "      # Remove a primeira linha que contém o número do registro\n",
        "      lines.pop(0)\n",
        "      # Junta as linhas restantes em um único texto\n",
        "      text = \" \".join(lines)\n",
        "      # Adiciona o texto à lista de textos\n",
        "      queries.append(text)\n",
        "\n",
        "\n",
        "# Load the relevance judgments into a dictionary: The code opens the file \"cisi.rel\" \n",
        "# reads the contents into a Python dictionary called relevance_judgments, \n",
        "# each query ID is associated with a list of relevant document IDs.\n",
        "\n",
        "relevance_judgments = {}\n",
        "with open(\"/content/drive/MyDrive/cisi/CISI.REL\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for i in range(0, len(lines)):\n",
        "        query_id, doc_id, _, _ = lines[i].split()\n",
        "        if query_id not in relevance_judgments:\n",
        "            relevance_judgments[query_id] = []\n",
        "        relevance_judgments[query_id].append(doc_id)\n",
        "\n",
        "# Funtions to pre process tokens \n",
        "# List of english stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = stopwords.words('english') \n",
        "\n",
        "# Defines a stemizator based on Porter algo.\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "def preprocess(text):\n",
        "  # Converte o texto para minúsculas\n",
        "  text = text.lower()\n",
        "  # Remove pontuação, caracteres especiais e números usando expressões regulares\n",
        "  text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "  text = re.sub(r\"\\d+\", \"\", text)\n",
        "  # Divide o texto em tokens (palavras) usando espaços em branco como separador\n",
        "  tokens = text.split()\n",
        "  # Remove os tokens que são stopwords usando list comprehension \n",
        "  tokens = [token for token in tokens if token not in stopwords]\n",
        "  # Aplica stemização aos tokens usando list comprehension \n",
        "  tokens = [stemmer.stem(token) for token in tokens]\n",
        "  # Retorna a lista de tokens pré-processados \n",
        "  return tokens\n",
        "\n",
        "# Define the BM25 ranking function\n",
        "# the BM25 ranking function: The code defines a function called bm25 that takes \n",
        "# as input and calculates the BM25 score for each document in the database. \n",
        "# and b parameters are set to default values of 1.2 and 0.75, respectively.\n",
        "# doc_text.lower().split()\n",
        "\n",
        "def bm25(query, doc_text, k1=1.2, b=0.75):\n",
        "    query_terms = preprocess(query)\n",
        "    scores = Counter()\n",
        "    for doc_id, doc_text in doc_text.items():\n",
        "        doc_terms = doc_text\n",
        "        doc_length = len(doc_terms)\n",
        "        doc_term_counts = Counter(doc_terms)\n",
        "        for term in query_terms:\n",
        "            if term in doc_term_counts:\n",
        "                idf = log(len(doc_text) / doc_term_counts[term])\n",
        "                tf = doc_term_counts[term]\n",
        "                score = idf * ((k1 + 1) * tf) / (k1 * ((1 - b) + b * (doc_length / len(doc_terms))) + tf)\n",
        "                scores[doc_id] += score\n",
        "    return scores\n",
        "\n",
        "\n",
        "\n",
        "# Calculates the bm25 scores for each document for each query.\n",
        "# stores in a list results with [N queries [N docs]]\n",
        "# results = [bm25(query=q, doc_text=doc_text) for q in queries]\n",
        "#len(results )\n",
        "#len(results[0])\n",
        "\n",
        "\n",
        "\n",
        "# bm25(query=queries[0], doc_text=doc_text)\n",
        "\n",
        "# Define the evaluation function\n",
        "# The code defines a function called evaluate the results of the ranking \n",
        "# (as a dictionary of document scores for each query) \n",
        "# relevance judgments as input, and calculates the precision, recall, \n",
        "# and F1 score of the ranking.\n",
        "\n",
        "def evaluate(results, relevance_judgments):\n",
        "    num_queries = len(relevance_judgments)\n",
        "    precision = 0.0\n",
        "    recall = 0.0\n",
        "    f1 = 0.0\n",
        "    for query_id, docs in relevance_judgments.items():\n",
        "        retrieved_docs = [str(doc_id) for doc_id, _ in results[query_id]]\n",
        "        relevant_docs = set(docs)\n",
        "        retrieved_and_relevant = relevant_docs.intersection(retrieved_docs)\n",
        "        precision += len(retrieved_and_relevant) / len(retrieved_docs)\n",
        "        recall += len(retrieved_and_relevant) / len(relevant_docs)\n",
        "    precision /= num_queries\n",
        "    recall /= num_queries\n",
        "    try:\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "    except ZeroDivisionError:\n",
        "        f1 = 0\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Perform the BM25 ranking for each query and evaluate the results\n",
        "# Loop over each queries list, and applies the bm25 function to calculate the document scores \n",
        "# for the query. \n",
        "# The results are stored in a dictionary called results. Each query ID is associated\n",
        "# with a list of the top 100 documents ranked by BM25 score. \n",
        "# Then the evaluate function is called to calculate the precision, recall, \n",
        "# score of the ranking, and the results are printed to the console.\n",
        "\n",
        "results = {}\n",
        "doc_text2 =  {k : preprocess(doc_text[k]) for k in doc_text}\n",
        "for i, query in enumerate(queries):\n",
        "    query_id = str(i+1)\n",
        "    scores = bm25(query, doc_text=doc_text2)\n",
        "    results[query_id] = scores.most_common(30)\n",
        "\n",
        "precision, recall, f1 = evaluate(results, relevance_judgments)\n",
        "\n",
        "print(\"Precision: {:.2f}%\".format(precision * 100))\n",
        "print(\"Recall: {:.2f}%\".format(recall * 100))\n",
        "print(\"F1: {:.2f}%\".format(f1 * 100))   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f627711b-5dad-40d9-c4db-7637efb36b8a",
        "id": "GptABltic0x1"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.8/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from rank_bm25) (1.21.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 15.92%\n",
            "Recall: 14.24%\n",
            "F1: 15.03%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alternative method using `gensin` scoring function"
      ],
      "metadata": {
        "id": "hG7JwpMER_wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.summarization import bm25\n",
        "\n",
        "doc_text2 = [preprocess(doc_text[k]) for k in doc_text]\n",
        "\n",
        "queries2 = [preprocess(query) for query in queries]\n",
        "\n",
        "bm25_obj = bm25.BM25(doc_text2)\n",
        "\n",
        "avr_idf = sum(list(bm25_obj.idf.values())) / len(bm25_obj.idf.values())\n",
        "\n",
        "scores2 = [bm25_obj.get_scores(document = q, average_idf = avr_idf) for q in queries2]\n",
        "\n",
        "results2 = {}\n",
        "for i in range(len(queries2)):\n",
        "    query_id = str(i+1)\n",
        "    scrs =  { str(i+1):scr for i, scr in enumerate( scores2[i])}\n",
        "    #print(scrs)\n",
        "    scrs = Counter(scrs)\n",
        "    #print(scrs)\n",
        "    #print(scrs.most_common(30))\n",
        "    results2[query_id] = scrs.most_common(30)\n",
        "\n",
        "precision2, recall2, f12 = evaluate(results2, relevance_judgments)\n",
        "\n",
        "\n",
        "print(\"Precision: {:.2f}%\".format(precision2 * 100))\n",
        "print(\"Recall: {:.2f}%\".format(recall2 * 100))\n",
        "print(\"F1: {:.2f}%\".format(f12 * 100))   \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IO-Dh02wFWLz",
        "outputId": "b6f689be-0866-41cc-89d9-a8d82eb660a7"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 24.82%\n",
            "Recall: 26.39%\n",
            "F1: 25.58%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "print(inspect.getsource(bm25_obj.get_score))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T21gEqYdtit4",
        "outputId": "305c8cfd-1790-4009-c59e-ffe9c6f75ad1"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    def get_score(self, document, index, average_idf):\n",
            "        \"\"\"Computes BM25 score of given `document` in relation to item of corpus selected by `index`.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        document : list of str\n",
            "            Document to be scored.\n",
            "        index : int\n",
            "            Index of document in corpus selected to score with `document`.\n",
            "        average_idf : float\n",
            "            Average idf in corpus.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        float\n",
            "            BM25 score.\n",
            "\n",
            "        \"\"\"\n",
            "        score = 0\n",
            "        for word in document:\n",
            "            if word not in self.f[index]:\n",
            "                continue\n",
            "            idf = self.idf[word] if self.idf[word] >= 0 else EPSILON * average_idf\n",
            "            score += (idf * self.f[index][word] * (PARAM_K1 + 1)\n",
            "                      / (self.f[index][word] + PARAM_K1 * (1 - PARAM_B + PARAM_B * self.doc_len[index] / self.avgdl)))\n",
            "        return score\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "- The F1 scores obtained in the first and second implementations of the Gensim-based algorithm were 0.15 and 0.25, respectively. The cause of the differences in the scores has not yet been identified.\n",
        " \n",
        "- The initial implementation of the algorithm appears to be consistent with prior research, as reported in a relevant literature source (see www.researchgate.net/publication/266857829_A_new_fuzzy_logic_based_ranking_function_for_efficient_Information_Retrieval_system).\n",
        "\n",
        "- In this study, I utilized the new version of Bing that has an integrated new chatGPT-based search feature. \n",
        "\n",
        "- The use of these tools was informative and yielded comprehensive and objective results. However, it should be noted that the code did not always run smoothly and some issues were encountered during the implementation process, such as errors in variable types or the assumption of certain data file structures. Close inspection of the code was necessary to address these issues.\n",
        "\n",
        "- Overall, this implementation was a valuable learning experience and proved to be an effective tool for developing coding skills. The exercise was thoroughly enjoyable and insightful.- \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RYvYt_VMwVDS"
      }
    }
  ]
}